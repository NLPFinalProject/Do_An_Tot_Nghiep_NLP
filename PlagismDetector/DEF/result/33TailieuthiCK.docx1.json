{
  "file1": [
    "C\u00e1i hay c\u1ee7a Spark l\u00e0 n\u00f3 c\u00f3 kh\u1ea3 n\u0103ng t\u00edch h\u1ee3p r\u1ea5t m\u1ea1nh m\u1ebd trong Hadoop Ecosystem, c\u00f3 th\u1ec3 x\u1eed l\u00fd d\u1eef li\u1ec7u d\u1ea1ng Batch/ Near realtime, c\u00f3 s\u1eb5n c\u00e1c API ph\u1ee5c v\u1ee5 Machine Learning/ Graph Processing ch\u1ea1y ph\u00e2n t\u00e1n v\u1edbi \u0111\u1ed9 \u1ed5n \u0111\u1ecbnh c\u1ef1c cao.",
    "1.Apache Spark History",
    "Apache Spark b\u1eaft \u0111\u1ea7u t\u1ea1i University of California, Berkeley v\u00e0o n\u0103m 2009 v\u1edbi t\u00ean l\u00e0 \u201cSpark research project\u201d, \u0111\u01b0\u1ee3c gi\u1edbi thi\u1ec7u public l\u1ea7n \u0111\u1ea7u ti\u00ean sau m\u1ed9t n\u0103m (2010) trong m\u1ed9t b\u00e0i b\u00e1o c\u00f3 t\u1ef1a \u0111\u1ec1 \u201cSpark: Cluster Computing with Working Sets\u201d vi\u1ebft b\u1edfi 5 nh\u00e0 nghi\u00ean c\u1ee9u c\u1ee7a AMPlab \u2014 UC Berkeley.",
    "V\u00e0o th\u1eddi \u0111i\u1ec3m \u0111\u00f3, Hadoop MapReduce l\u00e0 c\u00f4ng c\u1ee5 l\u1eadp tr\u00ecnh song song m\u1ea1nh m\u1ebd v\u00e0 c\u0169ng l\u00e0 d\u1ef1 \u00e1n open-source \u0111\u1ea7u ti\u00ean \u0111\u1ec3 x\u1eed l\u00fd x\u1eed l\u00fd song song d\u1eef li\u1ec7u tr\u00ean c\u00e1c c\u1ee5m servers v\u1edbi h\u00e0ng ng\u00e0n node.",
    "AMPlab \u0111\u00e3 l\u00e0m vi\u1ec7c v\u1edbi nhi\u1ec1u ng\u01b0\u1eddi d\u00f9ng MapReduce s\u1edbm \u0111\u1ec3 hi\u1ec3u nh\u1eefng l\u1ee3i \u00edch v\u00e0 nh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a m\u00f4 h\u00ecnh l\u1eadp tr\u00ecnh m\u1edbi n\u00e0y v\u00e0 do \u0111\u00f3 c\u00f3 th\u1ec3 t\u1ed5ng h\u1ee3p danh s\u00e1ch c\u00e1c v\u1ea5n \u0111\u1ec1 qua m\u1ed9t s\u1ed1 use cases v\u00e0 b\u1eaft \u0111\u1ea7u thi\u1ebft k\u1ebf general-computing platforms.",
    "Ngo\u00e0i ra, nh\u00f3m nghi\u00ean c\u1ee9u c\u0169ng \u0111\u00e3 l\u00e0m vi\u1ec7c v\u1edbi Hadoop users t\u1ea1i UC Berkeley \u0111\u1ec3 hi\u1ec3u nhu c\u1ea7u c\u1ee7a h\u1ecd \u0111\u1ed1i v\u1edbi n\u1ec1n t\u1ea3ng, c\u1ee5 th\u1ec3 l\u00e0 c\u00e1c nh\u00f3m \u0111ang h\u1ecdc m\u00e1y quy m\u00f4 l\u1edbn b\u1eb1ng thu\u1eadt to\u00e1n l\u1eb7p c\u1ea7n th\u1ef1c hi\u1ec7n multi passes d\u1eef li\u1ec7u.",
    "Trong su\u1ed1t qu\u00e1 tr\u00ecnh t\u00ecm hi\u1ec3u, nh\u00f3m nghi\u00ean c\u1ee9u nh\u1eadn ra 2 v\u1ea5n \u0111\u1ec1: Cluster Computing c\u00f3 ti\u1ec1m n\u0103ng ph\u00e1t tri\u1ec3n m\u1ea1nh m\u1ebd b\u1edfi c\u00e1c \u1ee9ng d\u1ee5ng m\u1edbi ho\u00e0n to\u00e0n c\u00f3 th\u1ec3 x\u00e2y d\u1ef1ng v\u00e0 s\u1eed d\u1ee5ng h\u1ec7 th\u1ed1ng, d\u1eef li\u1ec7u hi\u1ec7n c\u00f3. C\u00f4ng c\u1ee5 MapReduce khi\u1ebfn vi\u1ec7c x\u00e2y d\u1ef1ng c\u00e1c \u1ee9ng d\u1ee5ng l\u1edbn tr\u1edf n\u00ean kh\u00f3 kh\u0103n v\u00e0 kh\u00f4ng hi\u1ec7u qu\u1ea3.",
    "T\u1eeb \u0111\u00f3 nh\u00f3m quy\u1ebft \u0111\u1ecbnh ph\u00e1t tri\u1ec3n Spark v\u1edbi m\u1ee5c ti\u00eau c\u1ed1t l\u00f5i l\u00e0 gi\u00fap vi\u1ec7c x\u00e2y d\u1ef1ng \u1ee9ng d\u1ee5ng c\u00f3 th\u1ec3 ph\u00e1t tri\u1ec3n d\u1ec5 d\u00e0ng c\u00f9ng v\u1edbi kh\u1ea3 n\u0103ng scale-up linh ho\u1ea1t \u0111\u1ec3 x\u1eed l\u00fd t\u1eadp d\u1eef li\u1ec7u l\u1edbn v\u00e0 r\u1ea5t l\u1edbn, nh\u00f3m \u0111\u00e3 thi\u1ebft k\u1ebf m\u1ed9t API d\u1ef1a tr\u00ean Functional Programming c\u00f3 th\u1ec3 vi\u1ebft \u1ee9ng d\u1ee5ng m\u1ed9t c\u00e1ch ng\u1eafn g\u1ecdn.",
    "Sau \u0111\u00f3 tri\u1ec3n khai API n\u00e0y qua m\u1ed9t c\u00f4ng c\u1ee5 m\u1edbi c\u00f3 th\u1ec3 th\u1ef1c hi\u1ec7n chia s\u1ebb d\u1eef li\u1ec7u trong b\u1ed9 nh\u1edb hi\u1ec7u qu\u1ea3, cho ph\u00e9p t\u00e1i s\u1eed d\u1ee5ng qua c\u00e1c b\u01b0\u1edbc t\u00ednh to\u00e1n.",
    "Spark cung c\u1ea5p t\u1eadp h\u1ee3p c\u00e1c Computing Engine v\u00e0 Libraries cho vi\u1ec7c x\u1eed l\u00fd d\u1eef li\u1ec7u song song tr\u00ean h\u1ec7 th\u1ed1ng l\u00ean t\u1edbi h\u00e0ng ng\u00e0n Servers, ngo\u00e0i ra n\u00f3 c\u00f2n h\u1ed7 tr\u1ee3 nhi\u1ec1u ng\u00f4n ng\u1eef l\u1eadp tr\u00ecnh \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng r\u1ed9ng r\u00e3i (Python, Java, Scala v\u00e0 R), bao g\u1ed3m c\u00e1c th\u01b0 vi\u1ec7n cho c\u00e1c t\u00e1c v\u1ee5 kh\u00e1c nhau, t\u1eeb SQL \u0111\u1ebfn Streaming, Machine Learning v\u00e0 graph-parallel computation.",
    "N\u0103m 2013, d\u1ef1 \u00e1n Spark \u0111\u01b0\u1ee3c trao t\u1eb7ng cho Apache Software Foundation, open-source project n\u00e0y sau \u0111\u00f3 nh\u1eadn \u0111\u01b0\u1ee3c s\u1ef1 c\u00f4ng nh\u1eadn v\u00e0 \u0111\u00f3ng g\u00f3p m\u1ea1nh m\u1ebd c\u1ed9ng \u0111\u1ed3ng.",
    "Cu\u1ed1i c\u00f9ng, 26/5/2014, Apache release Spark ver1.0 s\u1eb5n s\u00e0ng apply trong m\u00f4i tr\u01b0\u1eddng production.",
    "The Spark Components:",
    "Spark Core: l\u00e0 engine th\u1ef1c thi chung l\u00e0m n\u1ec1n t\u1ea3ng cho Spark.",
    "T\u1ea5t c\u1ea3 c\u00e1c ch\u1ee9c n\u0103ng kh\u00e1c \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng d\u1ef1a tr\u00ean base l\u00e0 Spark Core.",
    "N\u00f3 cung c\u1ea5p kh\u1ea3 n\u0103ng t\u00ednh to\u00e1n tr\u00ean b\u1ed9 nh\u1edb RAM v\u00e0 c\u1ea3 b\u1ed9 d\u1eef li\u1ec7u tham chi\u1ebfu trong c\u00e1c h\u1ec7 th\u1ed1ng external storage.",
    "Spark SQL: l\u00e0 m\u1ed9t th\u00e0nh ph\u1ea7n n\u1eb1m tr\u00ean Spark Core, gi\u1edbi thi\u1ec7u m\u1ed9t kh\u00e1i ni\u1ec7m tr\u1eebu t\u01b0\u1ee3ng h\u00f3a d\u1eef li\u1ec7u m\u1edbi g\u1ecdi l\u00e0 SchemaRDD, cung c\u1ea5p h\u1ed7 tr\u1ee3 cho d\u1eef li\u1ec7u c\u00f3 c\u1ea5u tr\u00fac v\u00e0 b\u00e1n c\u1ea5u tr\u00fac.",
    "Spark Streaming: t\u1eadn d\u1ee5ng kh\u1ea3 n\u0103ng l\u1eadp l\u1ecbch memory-base c\u1ee7a Spark Core \u0111\u1ec3 th\u1ef1c hi\u1ec7n streaming analytics.",
    "N\u00f3 l\u1ea5y d\u1eef li\u1ec7u theo mini-batches v\u00e0 th\u1ef1c hi\u1ec7n c\u00e1c ph\u00e9p bi\u1ebfn \u0111\u1ed5i RDD (B\u1ed9 d\u1eef li\u1ec7u ph\u00e2n t\u00e1n c\u00f3 kh\u1ea3 n\u0103ng ph\u1ee5c h\u1ed3i) tr\u00ean c\u00e1c mini-batches d\u1eef li\u1ec7u \u0111\u00f3.",
    "MLlib (Machine Learning Library): l\u00e0 m\u1ed9t framework machine learning ph\u00e2n t\u00e1n tr\u00ean Spark t\u1eadn d\u1ee5ng kh\u1ea3 n\u0103ng t\u00ednh to\u00e1n t\u1ed1c \u0111\u1ed9 cao nh\u1edd distributed memory-based c\u1ee7a ki\u1ebfn \u200b\u200btr\u00fac Spark.",
    "GraphX: \u200b\u200bl\u00e0 m\u1ed9t framework x\u1eed l\u00fd \u0111\u1ed3 th\u1ecb ph\u00e2n t\u00e1n.",
    "N\u00f3 cung c\u1ea5p m\u1ed9t API \u0111\u1ec3 th\u1ef1c hi\u1ec7n t\u00ednh to\u00e1n bi\u1ec3u \u0111\u1ed3 c\u00f3 th\u1ec3 m\u00f4 h\u00ecnh h\u00f3a c\u00e1c bi\u1ec3u \u0111\u1ed3 do ng\u01b0\u1eddi d\u00f9ng x\u00e1c \u0111\u1ecbnh b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng API \u0111\u00e3 \u0111\u01b0\u1ee3c t\u1ed1i \u01b0u s\u1eb5n.",
    "Spark vs Hadoop MapReduce",
    "V\u1ec1 c\u01a1 ch\u1ebf ho\u1ea1t \u0111\u1ed9ng c\u1ee7a MapReduce (MR):",
    "Input data \u0111\u01b0\u1ee3c \u0111\u1ecdc t\u1eeb HDFS (component ph\u1ee5 tr\u00e1ch vi\u1ec7c l\u01b0u tr\u1eef trong Hadoop) \u2192 x\u1eed l\u00fd b\u1eb1ng c\u00e1c thao t\u00e1c ch\u1ec9 \u0111\u1ecbnh \u2192 output \u0111\u01b0\u1ee3c ghi v\u00e0o HDFS \u2192 data ti\u1ebfp t\u1ee5c \u0111\u01b0\u1ee3c load \u2192 thao t\u00e1c ti\u1ebfp theo \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n \u2192 output ti\u1ebfp t\u1ee5c ghi v\u00e0o HDFS \u2026",
    "chu\u1ed7i c\u00e1c step [read-process-write] \u0111\u00f3 \u0111\u01b0\u1ee3c l\u1eb7p cho \u0111\u1ebfn khi ho\u00e0n th\u00e0nh c\u00f4ng vi\u1ec7c.",
    "V\u00ec input \u0111\u01b0\u1ee3c chia th\u00e0nh c\u00e1c block \u0111\u1ed9c l\u1eadp v\u1edbi nhau, c\u00e1c task map-reduce \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n song song, n\u00ean v\u1ec1 c\u01a1 b\u1ea3n n\u00f3 h\u1eefu \u00edch \u0111\u1ec3 x\u1eed l\u00ed nh\u1eefng b\u1ed9 d\u1eef li\u1ec7u l\u1edbn.",
    "Tuy nhi\u00ean, MR v\u1eabn c\u00f2n nh\u1eefng t\u1ed3n t\u1ea1i l\u00e0 qu\u00e1 tr\u00ecnh x\u1eed l\u00fd kh\u00f4ng th\u1ef1c s\u1ef1 hi\u1ec7u qu\u1ea3 trong tr\u01b0\u1eddng h\u1ee3p ph\u1ea3i l\u1eb7p l\u1ea1i nhi\u1ec1u step, v\u00ec m\u1ed7i step c\u1ea7n thi\u1ebft ph\u1ea3i ghi output v\u00e0o HDFS tr\u01b0\u1edbc khi step ti\u1ebfp theo \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n \u2192 vi\u1ec7c n\u00e0y t\u1ea1o ra c\u00e1c v\u1ea5n \u0111\u1ec1 trong vi\u1ec7c l\u01b0u tr\u1eef v\u00e0 replicate, t\u0103ng \u0111\u1ed9 tr\u1ec5 x\u1eed l\u00fd do ph\u1ea7n l\u1edbn th\u1ef1c hi\u1ec7n tr\u00ean Disk v\u1ed1n c\u00f3 hi\u1ec7u su\u1ea5t I/O kh\u00f4ng cao.",
    "B\u00ean c\u1ea1nh \u0111\u00f3 l\u00e0 vi\u1ec7c develop, debug v\u1edbi MR c\u00f3 ph\u1ea7n kh\u00f3 kh\u0103n v\u00ec code d\u00e0i d\u00f2ng.",
    "C\u01a1 ch\u1ebf ho\u1ea1t \u0111\u1ed9ng c\u1ee7a Spark: kh\u1eafc ph\u1ee5c nh\u1eefng t\u1ed3n t\u1ea1i c\u1ee7a Hadoop MapReduce, Spark \u0111\u01b0a ra m\u1ed9t kh\u00e1i ni\u1ec7m m\u1edbi RDD \u2014 Resilient Distributed Dataset \u0111\u00f3ng vai tr\u00f2 nh\u01b0 1 c\u1ea5u tr\u00fac d\u1eef li\u1ec7u c\u01a1 b\u1ea3n trong Spark, RDD \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a l\u00e0 tr\u1eebu t\u01b0\u1ee3ng cho m\u1ed9t t\u1eadp h\u1ee3p c\u00e1c ph\u1ea7n t\u1eed b\u1ea5t bi\u1ebfn (b\u1ea3n ch\u1ea5t l\u00e0 \u0111\u01b0\u1ee3c l\u01b0u tr\u00ean c\u00e1c \u00f4 nh\u1edb read-only), \u0111\u01b0\u1ee3c ph\u00e2n v\u00f9ng c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c chia s\u1ebb, t\u00e1c \u0111\u1ed9ng song song.",
    "Qua \u0111\u00f3, input data t\u1eeb storage system ch\u1ec9 c\u1ea7n load 1 l\u1ea7n duy nh\u1ea5t, c\u00e1c step th\u1ef1c hi\u1ec7n bi\u1ebfn \u0111\u1ed5i, x\u1eed l\u00fd input data \u0111\u01b0\u1ee3c l\u00ean k\u1ebf ho\u1ea1ch, optimize v\u00e0 th\u1ef1c hi\u1ec7n m\u1ed9t c\u00e1ch li\u00ean t\u1ee5c cho \u0111\u1ebfn khi output \u0111\u01b0\u1ee3c tr\u1ea3 khi k\u1ebft th\u00fac c\u00f4ng vi\u1ec7c.",
    "To\u00e0n b\u1ed9 qu\u00e1 tr\u00ecnh \u0111\u00f3 \u0111\u01b0\u1ee3c di\u1ec5n ra tr\u00ean b\u1ed9 nh\u1edb RAM (khi h\u1ebft RAM s\u1ebd \u0111\u01b0\u1ee3c chuy\u1ec3n sang x\u1eed l\u00fd tr\u00ean Disk) t\u1eadn d\u1ee5ng \u0111\u01b0\u1ee3c hi\u1ec7u su\u1ea5t I/O cao t\u1eeb \u0111\u00f3 c\u00f3 th\u1ec3 gi\u1ea3m th\u1eddi gian th\u1ef1c thi nh\u1ecf h\u01a1n 10\u2013100 l\u1ea7n Hadoop MapReduce.",
    "V\u1edbi Apache Spark, c\u00e1c d\u1eef li\u1ec7u trung gian \u0111\u01b0\u1ee3c l\u01b0u tr\u1eef in-memory, trong c\u00e1c object RDD",
    "Apache Spark Pros & Cons",
    "T\u00ednh n\u0103ng v\u00e0 c\u0169ng l\u00e0 \u01b0u \u0111i\u1ec3m c\u1ee7a Spark c\u00f3 th\u1ec3 k\u1ec3 ra :",
    "Advanced Analytics: Spark kh\u00f4ng ch\u1ec9 h\u1ed7 tr\u1ee3 \u201cMap\u201d v\u00e0 \u201cReduce \u201c, n\u00f3 c\u00f2n h\u1ed7 tr\u1ee3 Spark truy v\u1ea5n SQL, Streaming data, Machine learning (ML) v\u00e0 c\u00e1c thu\u1eadt to\u00e1n x\u1eed l\u00fd \u0111\u1ed3 th\u1ecb \u0111\u00f3ng vai tr\u00f2 nh\u01b0 m\u1ed9t b\u1ed9 c\u00f4ng c\u1ee5 ph\u00e2n t\u00edch d\u1eef li\u1ec7u c\u1ef1c k\u00ec m\u1ea1nh m\u1ebd.",
    "Speed: Spark gi\u00fap ch\u1ea1y m\u1ed9t \u1ee9ng d\u1ee5ng v\u1edbi t\u1ed1c \u0111\u1ed9 r\u1ea5t nhanh.",
    "So v\u1edbi Hadoop cluster, Spark Application n\u1ebfn ch\u1ea1y tr\u00ean b\u1ed9 nh\u1edb nhanh h\u01a1n t\u1edbi 100 l\u1ea7n v\u00e0 nhanh h\u01a1n 10 l\u1ea7n khi ch\u1ea1y tr\u00ean \u0111\u0129a.",
    "\u0110i\u1ec1u n\u00e0y c\u00f3 \u0111\u01b0\u1ee3c nh\u1edd gi\u1ea3m s\u1ed1 l\u01b0\u1ee3ng c\u00e1c ho\u1ea1t \u0111\u1ed9ng \u0111\u1ecdc / ghi v\u00e0o \u1ed5 \u0111\u0129a.",
    "Supports multiple languages: Spark cung c\u1ea5p built-in APIs ph\u1ed5 bi\u1ebfn t\u1eeb Java, Scala \u0111\u1ebfn Python, R. Do \u0111\u00f3, c\u00f3 th\u1ec3 code Spark applications v\u1edbi nhi\u1ec1u l\u1ef1a ch\u1ecdn v\u1ec1 ng\u00f4n ng\u1eef l\u1eadp tr\u00ecnh.",
    "B\u00ean c\u1ea1nh \u0111\u00f3 Spark c\u00f2n cung c\u1ea5p r\u1ea5t nhi\u1ec1u high-level operators cho vi\u1ec7c truy v\u1ea5n d\u1eef li\u1ec7u\u2026",
    "Nh\u01b0\u1ee3c \u0111i\u1ec3m:",
    "Spark kh\u00f4ng c\u00f3 h\u1ec7 th\u1ed1ng Filesystem ri\u00eang, do \u0111\u00f3, n\u00f3 ph\u1ee5 thu\u1ed9c v\u00e0o m\u1ed9t s\u1ed1 n\u1ec1n t\u1ea3ng kh\u00e1c nh\u01b0 Hadoop ho\u1eb7c m\u1ed9t n\u1ec1n t\u1ea3ng d\u1ef1a tr\u00ean \u0111\u00e1m m\u00e2y (S3, Google Cloud Storage,\u2026).",
    "Apache Spark \u0111\u00f2i h\u1ecfi r\u1ea5t nhi\u1ec1u RAM \u0111\u1ec3 ch\u1ea1y trong b\u1ed9 nh\u1edb, do \u0111\u00f3 chi ph\u00ed c\u1ee7a Spark kh\u00e1 cao.",
    "Spark Streaming kh\u00f4ng th\u1ef1c s\u1ef1 real-time.",
    "Vi\u1ec7c t\u1ed1i \u01b0u h\u00f3a, tinh ch\u1ec9nh \u0111\u1ec3 ph\u00f9 h\u1ee3p v\u1edbi c\u00e1c b\u1ed9 d\u1eef li\u1ec7u c\u1ee5 th\u1ec3 c\u1ea7n c\u00f3 kinh nghi\u1ec7m v\u00e0 v\u1eabn c\u1ea7n th\u1ef1c hi\u1ec7n th\u1ee7 c\u00f4ng.",
    "Real-World Use cases",
    "Ebay: eBay s\u1eed d\u1ee5ng Apache Spark \u0111\u1ec3 cung c\u1ea5p c\u00e1c \u01b0u \u0111\u00e3i \u0111\u01b0\u1ee3c nh\u1eafm m\u1ee5c ti\u00eau, n\u00e2ng cao tr\u1ea3i nghi\u1ec7m c\u1ee7a kh\u00e1ch h\u00e0ng v\u00e0 \u0111\u1ec3 t\u1ed1i \u01b0u h\u00f3a hi\u1ec7u su\u1ea5t t\u1ed5ng th\u1ec3.",
    "Apache Spark \u0111\u01b0\u1ee3c t\u1eadn d\u1ee5ng t\u1ea1i eBay th\u00f4ng qua Hadoop YARN.Alibaba: Alibaba m\u1ed9t trong nh\u1eefng n\u1ec1n t\u1ea3ng th\u01b0\u01a1ng m\u1ea1i \u0111i\u1ec7n t\u1eed l\u1edbn nh\u1ea5t th\u1ebf gi\u1edbi, s\u1eed Apache Spark \u0111\u1ec3 ph\u00e2n t\u00edch h\u00e0ng tr\u0103m petabyte d\u1eef li\u1ec7u tr\u00ean n\u1ec1n t\u1ea3ng th\u01b0\u01a1ng m\u1ea1i \u0111i\u1ec7n t\u1eed c\u1ee7a m\u00ecnh.",
    "M\u1ed9t s\u1ed1 c\u00f4ng vi\u1ec7c Spark th\u1ef1c hi\u1ec7n tr\u00edch xu\u1ea5t t\u00ednh n\u0103ng tr\u00ean d\u1eef li\u1ec7u h\u00ecnh \u1ea3nh, ch\u1ea1y trong v\u00e0i tu\u1ea7n.",
    "H\u00e0ng tri\u1ec7u th\u01b0\u01a1ng nh\u00e2n v\u00e0 ng\u01b0\u1eddi d\u00f9ng t\u01b0\u01a1ng t\u00e1c v\u1edbi n\u1ec1n t\u1ea3ng th\u01b0\u01a1ng m\u1ea1i \u0111i\u1ec7n t\u1eed Alibaba Taobao.",
    "M\u1ed7i t\u01b0\u01a1ng t\u00e1c n\u00e0y \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n d\u01b0\u1edbi d\u1ea1ng m\u1ed9t bi\u1ec3u \u0111\u1ed3 l\u1edbn ph\u1ee9c t\u1ea1p v\u00e0 Spark \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 x\u1eed l\u00fd nhanh qu\u00e1 tr\u00ecnh b\u1eb1ng c\u00e1c thu\u1eadt to\u00e1n ML tinh vi tr\u00ean d\u1eef li\u1ec7u n\u00e0y.Nh\u1eefng use case ti\u00eau bi\u1ec3u kh\u00e1c c\u00f3 th\u1ec3 li\u1ec7t k\u00ea nh\u01b0 c\u00e1c c\u00f4ng ty c\u00f4ng ngh\u1ec7 nh\u01b0 Uber v\u00e0 Netflix s\u1eed d\u1ee5ng c\u00e1c c\u00f4ng c\u1ee5 Spark Streaming v\u00e0 MLlib, \u0111\u1ebfn c\u00e1c t\u1ed5 ch\u1ee9c nh\u01b0 NASA, CERN v\u00e0 Broad Institute of MIT v\u00e0 Harvard \u00e1p d\u1ee5ng Spark v\u00e0o ph\u00e2n t\u00edch d\u1eef li\u1ec7u khoa h\u1ecdc.",
    "Spark Application",
    "M\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh Spark s\u1ebd g\u1ed3m 2 th\u00e0nh ph\u1ea7n ch\u00ednh:",
    "Driver Program: L\u00e0 1 JVM Process, ch\u1ee9a h\u00e0m main() nh\u01b0 b\u1ea5t k\u00ec 1 ch\u01b0\u01a1ng tr\u00ecnh JVM n\u00e0o kh\u00e1c, n\u00f3 \u0111\u00f3ng vai tr\u00f2 \u0111i\u1ec1u ph\u1ed1i code/ logic x\u1eed l\u00fd tr\u00ean Driver.",
    "Driver program ch\u1ee9a SparkSession",
    "Executors: L\u00e0 c\u00e1c worker, ch\u1ecbu tr\u00e1ch nhi\u1ec7m th\u1ef1c hi\u1ec7n c\u00e1c t\u00ednh to\u00e1n c\u00e1c logic nh\u1eadn t\u1eeb Driver.",
    "D\u1eef li\u1ec7u c\u1ea7n x\u1eed l\u00fd c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c load tr\u1ef1c ti\u1ebfp v\u00e0o memory c\u1ee7a Executor.",
    "Spark session: \u0110\u1ea1i di\u1ec7n cho kh\u1ea3 n\u0103ng t\u01b0\u01a1ng t\u00e1c v\u1edbi executors trong 1 ch\u01b0\u01a1ng tr\u00ecnh.",
    "Spark session ch\u00ednh l\u00e0 entry point c\u1ee7a m\u1ecdi ch\u01b0\u01a1ng tr\u00ecnh Spark.",
    "T\u1eeb SparkSession, c\u00f3 th\u1ec3 t\u1ea1o RDD/ DataFrame/ DataSet, th\u1ef1c thi SQL\u2026 t\u1eeb \u0111\u00f3 th\u1ef1c thi t\u00ednh to\u00e1n ph\u00e2n t\u00e1n.",
    "Khi ch\u1ea1y, t\u1eeb logic c\u1ee7a ch\u01b0\u01a1ng tr\u00ecnh (ch\u00ednh l\u00e0 code x\u1eed l\u00fd th\u00f4ng qua vi\u1ec7c g\u1ecdi c\u00e1c API), Driver s\u1ebd sinh ra c\u00e1c task t\u01b0\u01a1ng \u1ee9ng v\u00e0 l\u00ean l\u1ecbch ch\u1ea1y c\u00e1c task, sau \u0111\u00f3 g\u1eedi xu\u1ed1ng Executor \u0111\u1ec3 th\u1ef1c thi.",
    "D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c l\u01b0u tr\u00ean memory c\u1ee7a Executor n\u00ean vi\u1ec7c th\u1ef1c thi t\u00ednh to\u00e1n s\u1ebd nhanh h\u01a1n r\u1ea5t nhi\u1ec1u.",
    "R\u00f5 r\u00e0ng l\u00e0 d\u1eef li\u1ec7u ph\u00e2n t\u00e1n r\u1eddi r\u1ea1c trong m\u1ea1ng, v\u1eady l\u00e0m sao \u0111\u1ec3 c\u00f3 th\u1ec3 t\u00e1c \u0111\u1ed9ng l\u00ean t\u1eadp d\u1eef li\u1ec7u n\u00e0y?",
    "Spark gi\u1edbi thi\u1ec7u 1 kh\u00e1i ni\u1ec7m l\u00e0 RDD (Resilient Distributed Dataset), d\u1ecbch th\u00f4 ra ti\u1ebfng vi\u1ec7t l\u00e0: T\u1eadp d\u1eef li\u1ec7u ph\u00e2n t\u00e1n linh ho\u1ea1t.",
    "Trong 1 ch\u01b0\u01a1ng tr\u00ecnh Spark, RDD l\u00e0 \u0111\u1ea1i di\u1ec7n cho t\u1eadp d\u1eef li\u1ec7u ph\u00e2n t\u00e1n.",
    "M\u1ed9t v\u00ed d\u1ee5 cho c\u00e1c b\u1ea1n m\u1edbi d\u1ec5 hi\u1ec3u nh\u00e9.",
    "Ta c\u00f3 1 object colors = Array {red, blue, black, white, yellow}.",
    "Trong ch\u01b0\u01a1ng tr\u00ecnh th\u00f4ng th\u01b0\u1eddng, ph\u1ea7n d\u1eef li\u1ec7u (red, blue, black, white, yellow) n\u1eb1m tr\u00ean 1 m\u00e1y t\u00ednh duy nh\u1ea5t v\u00e0 th\u00f4ng qua bi\u1ebfn colors, b\u1ea1n c\u00f3 th\u1ec3 truy c\u1eadp \u0111\u1ebfn d\u1eef li\u1ec7u.",
    "Tuy nhi\u00ean trong h\u1ec7 ph\u00e2n t\u00e1n, n\u1ebfu ph\u1ea7n d\u1eef li\u1ec7u red, blue n\u1eb1m tr\u00ean m\u00e1y t\u00ednh A c\u00f2n black, white, yellow l\u1ea1i n\u1eb1m tr\u00ean m\u00e1y t\u00ednh B th\u00ec sao?",
    "RDD s\u1ebd gi\u00fap b\u1ea1n truy c\u1eadp 2 ph\u1ea7n d\u1eef li\u1ec7u r\u1eddi r\u1ea1c n\u00e0y nh\u01b0 1 \u0111\u1ed1i t\u01b0\u1ee3ng th\u00f4ng th\u01b0\u1eddng.",
    "\u0110\u00f3 l\u00e0 l\u00fd do t\u1ea1i sao m\u00ecnh n\u00f3i RDD l\u00e0 \u0111\u1ea1i di\u1ec7n cho t\u1eadp d\u1eef li\u1ec7u ph\u00e2n t\u00e1n.",
    "\u0110i\u1ec1u n\u00e0y c\u00f3 ngh\u0129a l\u00e0, n\u00f3 l\u01b0u tr\u1eef tr\u1ea1ng th\u00e1i b\u1ed9 nh\u1edb nh\u01b0 m\u1ed9t \u0111\u1ed1i t\u01b0\u1ee3ng tr\u00ean c\u00e1c c\u00f4ng vi\u1ec7c v\u00e0 \u0111\u1ed1i t\u01b0\u1ee3ng c\u00f3 th\u1ec3 chia s\u1ebb gi\u1eefa c\u00e1c c\u00f4ng vi\u1ec7c \u0111\u00f3.",
    "Chia s\u1ebb d\u1eef li\u1ec7u trong b\u1ed9 nh\u1edb nhanh h\u01a1n m\u1ea1ng v\u00e0 \u0110\u0129a t\u1eeb 10 \u0111\u1ebfn 100 l\u1ea7n.",
    "T\u1ea5t c\u1ea3 c\u00f4ng vi\u1ec7c trong Spark \u0111\u01b0\u1ee3c th\u1ec3 hi\u1ec7n d\u01b0\u1edbi d\u1ea1ng t\u1ea1o RDD m\u1edbi, chuy\u1ec3n \u0111\u1ed5i RDD hi\u1ec7n c\u00f3 ho\u1eb7c g\u1ecdi c\u00e1c h\u00e0nh \u0111\u1ed9ng tr\u00ean RDD \u0111\u1ec3 t\u00ednh to\u00e1n k\u1ebft qu\u1ea3.",
    "Spark t\u1ef1 \u0111\u1ed9ng ph\u00e2n ph\u1ed1i d\u1eef li\u1ec7u c\u00f3 trong RDD tr\u00ean to\u00e0n b\u1ed9 c\u1ee5m c\u1ee7a b\u1ea1n v\u00e0 song song h\u00f3a c\u00e1c thao t\u00e1c b\u1ea1n th\u1ef1c hi\u1ec7n tr\u00ean ch\u00fang",
    "\u0110\u1eb7c \u0111i\u1ec3m quan tr\u1ecdng c\u1ee7a 1 RDD l\u00e0 s\u1ed1 partitions.",
    "M\u1ed9t RDD bao g\u1ed3m nhi\u1ec1u partition nh\u1ecf, m\u1ed7i partition n\u00e0y \u0111\u1ea1i di\u1ec7n cho 1 ph\u1ea7n d\u1eef li\u1ec7u ph\u00e2n t\u00e1n.",
    "Kh\u00e1i ni\u1ec7m partition l\u00e0 logical, t\u1ee9c l\u00e0 1 node x\u1eed l\u00fd c\u00f3 th\u1ec3 ch\u1ee9a nhi\u1ec1u h\u01a1n 1 RDD partition.",
    "Theo m\u1eb7c \u0111\u1ecbnh, d\u1eef li\u1ec7u c\u00e1c partitions s\u1ebd l\u01b0u tr\u00ean memory.",
    "Th\u1eed t\u01b0\u1edfng t\u01b0\u1ee3ng b\u1ea1n c\u1ea7n x\u1eed l\u00fd 1TB d\u1eef li\u1ec7u, n\u1ebfu l\u01b0u h\u1ebft tr\u00ean mem t\u00ednh ra th\u00ec cung kh\u00e1 t\u1ed1n k\u00e9m nh\u1ec9.",
    "T\u1ea5t nhi\u00ean n\u1ebfu b\u1ea1n c\u00f3 1TB ram \u0111\u1ec3 x\u1eed l\u00fd th\u00ec t\u1ed1t qu\u00e1 nh\u01b0ng \u0111i\u1ec1u \u0111\u00f3 kh\u00f4ng c\u1ea7n thi\u1ebft.",
    "V\u1edbi vi\u1ec7c chia nh\u1ecf d\u1eef li\u1ec7u th\u00e0nh c\u00e1c partition v\u00e0 c\u01a1 ch\u1ebf lazy evaluation c\u1ee7a Spark b\u1ea1n c\u00f3 th\u1ec3 ch\u1ec9 c\u1ea7n v\u00e0i ch\u1ee5c GB ram v\u00e0 1 ch\u01b0\u01a1ng tr\u00ecnh \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf t\u1ed1t \u0111\u1ec3 x\u1eed l\u00fd 1TB d\u1eef li\u1ec7u, ch\u1ec9 l\u00e0 s\u1ebd ch\u1eadm h\u01a1n c\u00f3 nhi\u1ec1u RAM th\u00f4i.",
    "M\u1ed7i Executor c\u00f3 th\u1ec3 ch\u1ee9a d\u1eef li\u1ec7u c\u1ee7a 1 ho\u1eb7c 1 v\u00e0i partition c\u1ee7a 1 RDD.",
    "T\u1ea1o RDD:",
    "L\u1ea5y c\u00e1c t\u1ec7p d\u1eef li\u1ec7u: #Creating a RDD from a file",
    "data_file = \"./kddcup.data_10_percent.gz\"",
    "raw_data = sc.textFile(data_file)",
    "#Creating RDD using parallelize from an already existing list.",
    "a = range(100)",
    "data = sc.parallelize(a)",
    "C\u01a1 b\u1ea3n v\u1ec1 RDD: # The filter transformation",
    "normal_raw_data = raw_data.filter(lambda x: 'normal.'",
    "normal_count = normal_raw_data.count() # The map transformation",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))",
    "head_rows = csv_data.take(5) # Using map and predefined functions",
    "def parse_interaction(line):",
    "    elems = line.split(\",\")",
    "    tag = elems[41]",
    "    return (tag, elems)",
    "key_csv_data = raw_data.map(parse_interaction)",
    "head_rows = key_csv_data.take(5)",
    "# The collect action",
    "all_raw_data = raw_data.collect()",
    "Thu th\u1eadp t\u1ea5t c\u1ea3 c\u00e1c normal t\u01b0\u01a1ng t\u00e1c d\u01b0\u1edbi d\u1ea1ng c\u00e1c c\u1eb7p kh\u00f3a-gi\u00e1 tr\u1ecb: # get data from file",
    "data_file = \"./kddcup.data_10_percent.gz\"",
    "raw_data = sc.textFile(data_file)# parse into key-value pairs",
    "key_csv_data = raw_data.map(parse_interaction)# filter normal key interactions",
    "normal_key_interactions = key_csv_data.filter(lambda x: x[0] == \"normal.",
    "\")# collect all",
    "all_normal = normal_key_interactions.collect()",
    "normal_count = len(all_normal)",
    "print(\"There are {} 'normal' interactions\".format(normal_count))",
    "#The sample transformation",
    "raw_data_sample = raw_data.sample(False, 0.1, 1234)",
    "sample_size = raw_data_sample.count()",
    "total_size = raw_data.count()",
    "print(\"Sample size is {} of {}\".format(sample_size, total_size)) Output :",
    "Sample size is 489957 of 4898431 #The takeSample action",
    "raw_data_sample = raw_data.takeSample(False, 400000, 1234)",
    "normal_data_sample = [x.split(\",\") for x in raw_data_sample if \"normal.\"",
    "normal_sample_size = len(normal_data_sample)",
    "normal_ratio = normal_sample_size / 400000.0",
    "print(\"The ratio of 'normal' interactions is {}\".format(normal_ratio)) The ratio is 0.1988025",
    "S\u1eed d\u1ee5ng reduce: # parse data",
    "csv_data = raw_data.map(lambda x: x.split(\",\")) # separate into different RDDs",
    "normal_csv_data = csv_data.filter(lambda x: x[41]==\"normal.\")",
    "attack_csv_data = csv_data.filter(lambda x: x[41]!=\"normal.\")",
    "normal_duration_data = normal_csv_data.map(lambda x: int(x[0]))",
    "attack_duration_data = attack_csv_data.map(lambda x: int(x[0]))",
    "total_normal_duration = normal_duration_data.reduce(lambda x, y: x + y)",
    "total_attack_duration = attack_duration_data.reduce(lambda x, y: x + y)",
    "print(\"Total duration for 'normal' interactions is {}\".format(total_normal_duration))",
    "print(\"Total duration for 'attack' interactions is {}\".format(total_attack_duration))",
    "normal_count = normal_duration_data.count()",
    "attack_count = attack_duration_data.count()",
    "print(\"Meanduration_for_'normal'_interactions_is{}\".format(round(total_normal_duration/float(normal_count),3)))print(\"Mean_duration_for_'attack'_interactions is{}\"\u2026 (attack_count),3)))",
    "S\u1eed d\u1ee5ng aggregate : (0,0), # the initial value",
    "    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc",
    "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators",
    "attack_sum_count = attack_duration_data.aggregate(",
    "    (0,0), # the initial value",
    "    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc",
    "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators",
    ") / print(\u2026)",
    "L\u00e0m vi\u1ec7c v\u1edbi RDD c\u1ee7a c\u1eb7p kh\u00f3a / gi\u00e1 tr\u1ecb: T\u1ea1o m\u1ed9t c\u1eb7p RDD:",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))",
    "key_value_data = csv_data.map(lambda x: (x[41], x)) # x[41] contains the network interaction tag",
    "T\u1ed5ng h\u1ee3p d\u1eef li\u1ec7u v\u1edbi RDD c\u1ee7a c\u1eb7p kh\u00f3a / gi\u00e1 tr\u1ecb: S\u1eed d\u1ee5ng reduceByKey :",
    "key_value_duration = csv_data.map(lambda x: (x[41], float(x[0])))",
    "durations_by_key = key_value_duration.reduceByKey(lambda x, y: x + y)",
    "durations_by_key.collect()",
    "counts_by_key = key_value_data.countByKey()",
    "counts_by_key Output :",
    "defaultdict(<type 'int'>, {u'guess_passwd.",
    "': 53, u'nmap.",
    "': 231, u'warezmaster.",
    "': 20, u'rootkit.",
    "': 10, ..)",
    "S\u1eed d\u1ee5ng combineByKey : Ch\u00fang ta c\u00f3 th\u1ec3 coi n\u00f3 l\u00e0 aggregate t\u01b0\u01a1ng \u0111\u01b0\u01a1ng v\u00ec n\u00f3 cho ph\u00e9p ng\u01b0\u1eddi d\u00f9ng tr\u1ea3 v\u1ec1 c\u00e1c gi\u00e1 tr\u1ecb kh\u00f4ng c\u00f9ng lo\u1ea1i v\u1edbi d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o c\u1ee7a ch\u00fang ta.",
    "sum_counts = key_value_duration.combineByKey(",
    "    (lambda x: (x, 1)), # the initial value, with value x and count 1",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators",
    ")sum_counts.collectAsMap()",
    "duration_means_by_type = sum_counts.map(lambda (key,value): (key, round(value[0]/value[1],3))).collectAsMap() # Print them sorted",
    "for tag in sorted(duration_means_by_type, key=duration_means_by_type.get, reverse=True):",
    "    print(tag, duration_means_by_type[tag]) Output: portsweep.",
    "1915.299\u2026",
    "Spark SQL: x\u1eed l\u00fd c\u00f3 c\u1ea5u tr\u00fac \u0111\u1ec3 ph\u00e2n t\u00edch d\u1eef li\u1ec7u",
    "V\u1ec1 c\u01a1 b\u1ea3n, m\u1ecdi th\u1ee9 \u0111\u1ec1u xoay quanh kh\u00e1i ni\u1ec7m Khung d\u1eef li\u1ec7u v\u00e0 s\u1eed d\u1ee5ng ng\u00f4n ng\u1eef SQL \u0111\u1ec3 truy v\u1ea5n ch\u00fang.",
    "Ch\u00fang ta s\u1ebd th\u1ea5y c\u00e1ch tr\u1eebu t\u01b0\u1ee3ng h\u00f3a khung d\u1eef li\u1ec7u, r\u1ea5t ph\u1ed5 bi\u1ebfn trong c\u00e1c h\u1ec7 sinh th\u00e1i ph\u00e2n t\u00edch d\u1eef li\u1ec7u kh\u00e1c (v\u00ed d\u1ee5 nh\u01b0 R v\u00e0 Python / Pandas), n\u00f3 r\u1ea5t m\u1ea1nh khi th\u1ef1c hi\u1ec7n ph\u00e2n t\u00edch d\u1eef li\u1ec7u kh\u00e1m ph\u00e1.",
    "Tr\u00ean th\u1ef1c t\u1ebf, r\u1ea5t d\u1ec5 d\u00e0ng \u0111\u1ec3 di\u1ec5n \u0111\u1ea1t c\u00e1c truy v\u1ea5n d\u1eef li\u1ec7u khi \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng c\u00f9ng v\u1edbi ng\u00f4n ng\u1eef SQL.",
    "H\u01a1n n\u1eefa, Spark ph\u00e2n ph\u1ed1i c\u1ea5u tr\u00fac d\u1eef li\u1ec7u d\u1ef1a tr\u00ean c\u1ed9t n\u00e0y m\u1ed9t c\u00e1ch minh b\u1ea1ch, \u0111\u1ec3 l\u00e0m cho qu\u00e1 tr\u00ecnh truy v\u1ea5n hi\u1ec7u qu\u1ea3 nh\u1ea5t c\u00f3 th\u1ec3.",
    "L\u1ea5y khung d\u1eef li\u1ec7u:",
    "Spark DataFramel\u00e0 m\u1ed9t t\u1eadp h\u1ee3p d\u1eef li\u1ec7u ph\u00e2n t\u00e1n \u0111\u01b0\u1ee3c t\u1ed5 ch\u1ee9c th\u00e0nh c\u00e1c c\u1ed9t \u0111\u01b0\u1ee3c \u0111\u1eb7t t\u00ean.",
    "V\u1ec1 m\u1eb7t kh\u00e1i ni\u1ec7m, n\u00f3 t\u01b0\u01a1ng \u0111\u01b0\u01a1ng v\u1edbi m\u1ed9t b\u1ea3ng trong c\u01a1 s\u1edf d\u1eef li\u1ec7u quan h\u1ec7 ho\u1eb7c m\u1ed9t khung d\u1eef li\u1ec7u trong R ho\u1eb7c Pandas.",
    "Ch\u00fang c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng t\u1eeb nhi\u1ec1u ngu\u1ed3n nh\u01b0 RDD hi\u1ec7n c\u00f3 trong tr\u01b0\u1eddng h\u1ee3p c\u1ee7a ch\u00fang t\u00f4i.",
    "\u0110i\u1ec3m v\u00e0o t\u1ea5t c\u1ea3 c\u00e1c ch\u1ee9c n\u0103ng SQL trong Spark l\u00e0 SQLContextl\u1edbp.",
    "\u0110\u1ec3 t\u1ea1o m\u1ed9t th\u1ec3 hi\u1ec7n c\u01a1 b\u1ea3n, t\u1ea5t c\u1ea3 nh\u1eefng g\u00ec ch\u00fang ta c\u1ea7n l\u00e0 m\u1ed9t SparkContexttham chi\u1ebfu.",
    "V\u00ec ch\u00fang t\u00f4i \u0111ang ch\u1ea1y Spark \u1edf ch\u1ebf \u0111\u1ed9 shell (s\u1eed d\u1ee5ng pySpark), ch\u00fang t\u00f4i c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng \u0111\u1ed1i t\u01b0\u1ee3ng ng\u1eef c\u1ea3nh to\u00e0n c\u1ee5c sccho m\u1ee5c \u0111\u00edch n\u00e0y.",
    "Spark SQL c\u00f3 th\u1ec3 chuy\u1ec3n \u0111\u1ed5i RDD c\u1ee7a Rowc\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng th\u00e0nh a DataFrame.",
    "C\u00e1c h\u00e0ng \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng b\u1eb1ng c\u00e1ch chuy\u1ec3n m\u1ed9t danh s\u00e1ch c\u00e1c c\u1eb7p kh\u00f3a / gi\u00e1 tr\u1ecb d\u01b0\u1edbi d\u1ea1ng kwargs cho Rowl\u1edbp.",
    "C\u00e1c ph\u00edm x\u00e1c \u0111\u1ecbnh t\u00ean c\u1ed9t v\u00e0 c\u00e1c lo\u1ea1i \u0111\u01b0\u1ee3c suy ra b\u1eb1ng c\u00e1ch nh\u00ecn v\u00e0o h\u00e0ng \u0111\u1ea7u ti\u00ean.",
    "Do \u0111\u00f3, \u0111i\u1ec1u quan tr\u1ecdng l\u00e0 kh\u00f4ng c\u00f3 d\u1eef li\u1ec7u b\u1ecb thi\u1ebfu trong h\u00e0ng \u0111\u1ea7u ti\u00ean c\u1ee7a RDD \u0111\u1ec3 c\u00f3 th\u1ec3 suy ra l\u01b0\u1ee3c \u0111\u1ed3 m\u1ed9t c\u00e1ch ch\u00ednh x\u00e1c.",
    "Trong tr\u01b0\u1eddng h\u1ee3p c\u1ee7a ch\u00fang ta, tr\u01b0\u1edbc ti\u00ean ch\u00fang ta c\u1ea7n t\u00e1ch d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c ph\u00e2n t\u00e1ch b\u1eb1ng d\u1ea5u ph\u1ea9y, r\u1ed3i s\u1eed d\u1ee5ng th\u00f4ng tin trong m\u00f4 t\u1ea3 nhi\u1ec7m v\u1ee5 1999 c\u1ee7a KDD \u0111\u1ec3 l\u1ea5y t\u00ean c\u1ed9t .",
    "from pyspark.sql import SQLContext, Row",
    "sqlContext = SQLContext(sc)",
    "#Inferring the schema",
    "csv_data = raw_data.map(lambda l: l.split(\",\"))",
    "row_data = csv_data.map(lambda p: Row(duration=int(p[0]), protocol_type=p[1],",
    "    service=p[2],    flag=p[3],src_bytes=int(p[4]),dst_bytes=int(p[5]) ) )",
    "#Once we have our RDD of Row we can infer and register the schema.",
    "interactions_df = sqlContext.createDataFrame(row_data)",
    "interactions_df.registerTempTable(\"interactions\").",
    "Truy v\u1ea5n SQL qua khung d\u1eef li\u1ec7u d\u01b0\u1edbi d\u1ea1ng b\u1ea3ng.",
    "# Select tcp network interactions with more than 1 second duration and no transfer from destination",
    "tcp_interactions = sqlContext.sql(\"\"\" SELECT duration, dst_bytes FROM interactions WHERE protocol_type = 'tcp' AND duration > 1000 AND dst_bytes = 0 \"\"\")",
    "tcp_interactions.show() Output: duration dst_bytes 5057     0 \u2026",
    "K\u1ebft qu\u1ea3 c\u1ee7a c\u00e1c truy v\u1ea5n SQL l\u00e0 RDD v\u00e0 h\u1ed7 tr\u1ee3 t\u1ea5t c\u1ea3 c\u00e1c ho\u1ea1t \u0111\u1ed9ng RDD b\u00ecnh th\u01b0\u1eddng.",
    "# Output duration together with dst_bytes: tcp_interactions_out = tcp_interactions.map(lambda p: \"Duration: {}, Dest.",
    "bytes: {}\".format(p.duration, p.dst_bytes))",
    "for ti_out in tcp_interactions_out.collect():",
    "  print ti_out",
    "Truy v\u1ea5n d\u01b0\u1edbi d\u1ea1ng DataFrameho\u1ea1t \u0111\u1ed9ng: Spark DataFrame cung c\u1ea5p m\u1ed9t ng\u00f4n ng\u1eef d\u00e0nh ri\u00eang cho mi\u1ec1n \u0111\u1ec3 thao t\u00e1c d\u1eef li\u1ec7u c\u00f3 c\u1ea5u tr\u00fac.",
    "Ng\u00f4n ng\u1eef n\u00e0y bao g\u1ed3m c\u00e1c ph\u01b0\u01a1ng th\u1ee9c m\u00e0 ch\u00fang ta c\u00f3 th\u1ec3 gh\u00e9p n\u1ed1i \u0111\u1ec3 th\u1ef1c hi\u1ec7n vi\u1ec7c l\u1ef1a ch\u1ecdn, l\u1ecdc, nh\u00f3m, v.v. V\u00ed d\u1ee5, gi\u1ea3 s\u1eed ch\u00fang ta mu\u1ed1n \u0111\u1ebfm c\u00f3 bao nhi\u00eau t\u01b0\u01a1ng t\u00e1c cho m\u1ed7i lo\u1ea1i giao th\u1ee9c.",
    "Ch\u00fang ta c\u00f3 th\u1ec3 ti\u1ebfn h\u00e0nh nh\u01b0 sau.",
    "Gi\u1ea3 s\u1eed ch\u00fang ta mu\u1ed1n \u0111\u1ebfm c\u00f3 bao nhi\u00eau t\u01b0\u01a1ng t\u00e1c cho m\u1ed7i lo\u1ea1i giao th\u1ee9c.",
    "Ch\u00fang ta c\u00f3 th\u1ec3 ti\u1ebfn h\u00e0nh nh\u01b0 sau.",
    "interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").groupBy(\"protocol_type\").count().show()",
    "print(\"Query performed in {} seconds\".format(round(tt,3))) Output: protocol_type count: udp 20354 tcp           190065 icmp   283602",
    "Lazy Evalution: Transformation",
    "Vi\u1ec7c x\u1eed l\u00fd d\u1eef li\u1ec7u, nh\u00ecn r\u1ed9ng ra, ch\u00ednh l\u00e0 vi\u1ec7c bi\u1ebfn \u0111\u1ed5i t\u1eeb t\u1eadp d\u1eef li\u1ec7u n\u00e0y sang t\u1eadp d\u1eef li\u00eau kh\u00e1c, hay v\u1edbi Spark, l\u00e0 bi\u1ebfn \u0111\u1ed5i t\u1eeb RDD n\u00e0y sang RDD kh\u00e1c.",
    "VD b\u1ea1n c\u00f3 1 t\u1eadp web log r\u1ea5t l\u1edbn g\u1ed3m c\u1ea3 log call t\u1eeb app v\u00e0 web, c\u1ea7n t\u00ecm xem s\u1ed1 l\u01b0\u1ee3ng log s\u1eed d\u1ee5ng tr\u00ean app l\u00e0 bao nhi\u00eau, t\u1eeb t\u1eadp d\u1eef li\u1ec7u ban \u0111\u1ea7u s\u1ebd l\u1ecdc ra t\u1eadp d\u1eef li\u1ec7u nh\u1ecf h\u01a1n ch\u1ec9 ch\u1ee9a log call tr\u00ean di \u0111\u1ed9ng, r\u1ed3i count tr\u00ean t\u1eadp d\u1eef li\u1ec7u \u0111\u00f3.",
    "\u0110\u00f3 ch\u00ednh l\u00e0 minh h\u1ecda cho vi\u1ec7c bi\u1ebfn \u0111\u1ed5i c\u00e1c t\u1eadp d\u1eef li\u1ec7u.",
    "L\u00e0m vi\u1ec7c v\u1edbi RDD, Spark c\u00f3 2 lo\u1ea1i operations l\u00e0 Transformation v\u00e0 Actions.",
    "Ph\u00e9p bi\u1ebfn \u0111\u1ed5i t\u1eeb RDD n\u00e0y sang RDD kh\u00e1c l\u00e0 1 transformation, nh\u01b0 vi\u1ec7c bi\u1ebfn \u0111\u1ed5i t\u1eadp web log ban \u0111\u1ea7u sang t\u1eadp web log ch\u1ec9 ch\u1ee9a log g\u1ecdi qua app l\u00e0 1 transformation.",
    "Action: Sau t\u1ea5t c\u1ea3 c\u00e1c ph\u00e9p bi\u1ebfn \u0111\u1ed5i, khi mu\u1ed1n t\u01b0\u01a1ng t\u00e1c v\u1edbi k\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng (VD xem k\u1ebft qu\u1ea3, collect k\u1ebft qu\u1ea3, ghi k\u1ebft qu\u1ea3\u2026) ta g\u1ecdi 1 action.",
    "Lazy evaluation Khi th\u1ef1c thi, vi\u1ec7c g\u1ecdi c\u00e1c transformations, Spark s\u1ebd kh\u00f4ng ngay l\u1eadp t\u1ee9c th\u1ef1c thi c\u00e1c t\u00ednh to\u00e1n m\u00e0 s\u1ebd l\u01b0u l\u1ea1i th\u00e0nh 1 lineage, t\u1ee9c l\u00e0 t\u1eadp h\u1ee3p c\u00e1c bi\u1ebfn \u0111\u1ed5i t\u1eeb RDD n\u00e0y th\u00e0nh RDD kh\u00e1c qua m\u1ed7i transformation.",
    "Khi c\u00f3 1 action \u0111\u01b0\u1ee3c g\u1ecdi, Spark l\u00fac n\u00e0y m\u1edbi th\u1ef1c s\u1ef1 th\u1ef1c hi\u1ec7n c\u00e1c bi\u1ebfn \u0111\u1ed5i \u0111\u1ec3 tr\u1ea3 ra k\u1ebft qu\u1ea3.",
    "N\u1ebfu g\u1ecdi \u0111\u1ebfn \u0111\u00e2u ch\u1ea1y \u0111\u1ebfn \u0111\u1ea5y th\u00ec sau b\u01b0\u1edbc \u0111\u1ea7u ti\u00ean, \u1ee9ng d\u1ee5ng c\u1ea7n load s\u1eb5n 1TB ram v\u00e0o Mem, s\u1eb5n s\u00e0ng th\u1ef1c hi\u1ec7n b\u01b0\u1edbc ti\u1ebfp theo.",
    "Tuy nhi\u00ean, n\u1ebfu lazy, Driver s\u1ebd c\u00f3 \u201cc\u00e1i nh\u00ecn\u201d to\u1ea3n c\u1ea3nh t\u1eeb \u0111\u1ea7u \u0111\u1ebfn cu\u1ed1i v\u1ec1 output \u0111\u1ea7u ra, l\u00fac n\u00e0y Driver s\u1ebd sinh c\u00e1c task \u0111\u1ecdc t\u1eebng ph\u1ea7n nh\u1ecf c\u1ee7a 1TB, th\u1ef1c hi\u1ec7n l\u1ecdc tr\u00ean t\u1eadp d\u1eef li\u1ec7u nh\u1ecf v\u00e0 gi\u1eef l\u1ea1i k\u1ebft qu\u1ea3 count b\u1ea3n ghi l\u00e0 log app, load d\u1ea7n d\u1ea7n cho t\u1edbi khi h\u1ebft 1TB th\u00ec sum t\u1ed5ng c\u00e1c ph\u1ea7n nh\u1ecf l\u1ea1i s\u1ebd ra \u0111\u01b0\u1ee3c ph\u1ea7n l\u1edbn.",
    "Nh\u01b0 v\u1eady ta ch\u1ec9 c\u1ea7n l\u01b0\u1ee3ng ram nh\u1ecf nh\u01b0ng v\u1eabn c\u00f3 th\u1ec3 x\u1eed l\u00fd \u0111\u01b0\u1ee3c l\u01b0\u1ee3ng d\u1eef li\u1ec7u l\u1edbn g\u1ea5p nhi\u1ec1u l\u1ea7n.",
    "SparkSQL Apache Spark l\u00e0 framework x\u1eed l\u00fd x\u01b0 li\u1ec7u ph\u00e2n t\u00e1n, t\u1eadp h\u1ee3p nhi\u1ec1u th\u01b0 vi\u1ec7n cho c\u00e1c m\u1ee5c \u0111\u00edch kh\u00e1c nhau v\u00e0 build on-top Spark Core.",
    "\u0110i\u1ec1u n\u00e0y gi\u00fap Spark c\u00f3 t\u00ednh nh\u1ea5t qu\u00e1n l\u1edbn, thay v\u00ec m\u1ed7i m\u1ee5c \u0111\u00edch b\u1ea1n ph\u1ea3i t\u00ecm 1 th\u01b0 vi\u1ec7n ri\u00eang th\u00ec v\u1edbi vi\u1ec7c c\u00f3 s\u1eb5n nh\u01b0 Spark, trong 1 project b\u1ea1n v\u1eeba c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng Spark Core k\u1ebft h\u1ee3p v\u1edbi Spark SQL hay Mllib m\u1ed9t c\u00e1ch linh ho\u1ea1t, t\u1ed1i \u01b0u c\u00e1c c\u00f4ng vi\u1ec7c theo \u0111\u00fang m\u1ee5c \u0111\u00edch c\u1ee7a ch\u00fang.",
    "Khi th\u1ef1c thi, Spark SQL v\u1eabn s\u1ebd g\u1ecdi xu\u1ed1ng l\u1edbp Core b\u00ean d\u01b0\u1edbi, s\u1eed d\u1ee5ng RDD \u0111\u1ec3 t\u00ednh to\u00e1n.",
    "C\u00f3 th\u1ec3 t\u00f3m g\u1ecdn m\u1ed9t s\u1ed1 \u0111\u1eb7c \u0111i\u1ec3m quan tr\u1ecdng c\u1ee7a Spark SQL nh\u01b0 sau:",
    "\u0110\u01b0\u1ee3c x\u00e2y d\u1ef1ng ph\u00eda tr\u00ean t\u1ea7ng Spark Core, th\u1eeba h\u01b0\u1edfng t\u1ea5t c\u1ea3 c\u00e1c t\u00ednh n\u0103ng m\u00e0 RDD c\u00f3.",
    "L\u00e0m vi\u1ec7c v\u1edbi t\u1eadp d\u1eef li\u1ec7u l\u00e0 DataSet ho\u1eb7c DataFrame (t\u1eadp d\u1eef li\u1ec7u ph\u00e2n t\u00e1n, c\u00f3 c\u1ea5u tr\u00fac)",
    "Hi\u1ec7u n\u0103ng cao, kh\u1ea3 n\u0103ng m\u1edf r\u1ed9ng v\u00e0 ch\u1ecbu l\u1ed7i t\u1ed1t",
    "T\u01b0\u01a1ng t\u00edch v\u1edbi c\u00e1c th\u00e0nh ph\u1ea7n kh\u00e1c trong t\u1ed5ng th\u1ec3 Spark Framework (nh\u01b0 Streaming/ Mllib, GraphX)",
    "Bao g\u1ed3m 2 th\u00e0nh ph\u1ea7n l\u00e0 DataSet API v\u00e0 Catalyst Optimizer.",
    "Spark SQL Performance Kh\u00f4ng qu\u00e1 khi n\u00f3i r\u1eb1ng, trong vi\u1ec7c bi\u1ebfn \u0111\u1ed5i v\u00e0 t\u1ed5ng h\u1ee3p d\u1eef li\u1ec7u, Spark SQL v\u1edbi DataFrame lu\u00f4n c\u00f3 hi\u1ec7u n\u0103ng cao h\u01a1n r\u1ea5t nhi\u1ec1u so v\u1edbi s\u1eed dung RDD.",
    "C\u00f2n so v\u1edbi c\u00e1c c\u00f4ng ngh\u1ec7 t\u01b0\u01a1ng t\u1ef1 kh\u00e1c nh\u01b0 Impala hay Shark th\u00ec th\u1eddi gian th\u1ef1c thi v\u1edbi Spark SQL c\u0169ng lu\u00f4n r\u1ea5t \u1ea5n t\u01b0\u1ee3ng.",
    "DataFrame V\u1eady l\u00e0 c\u00f3 th\u1ec3 th\u1ea5y Spark SQL th\u1ef1c s\u1ef1 hi\u1ec7u qu\u1ea3 trong vi\u1ec7c x\u1eed l\u00fd d\u1eef li\u1ec7u nh\u01b0 n\u00e0o.",
    "C\u1ed1t l\u00f5i c\u1ee7a Spark SQL ch\u00ednh l\u00e0 DataFrame v\u00e0 c\u00e1c API t\u01b0\u01a1ng t\u00e1c.",
    "V\u1eady DataFrame l\u00e0 g\u00ec?",
    "Hi\u1ec3u \u0111\u01a1n gi\u1ea3n DF gi\u1ed1ng nh\u01b0 1 b\u1ea3ng trong h\u1ec7 CSDL quan h\u1ec7.",
    "C\u00f3 c\u00e1c tr\u01b0\u1eddng, \u0111\u01b0\u1ee3c \u0111\u1ecbnh s\u1eb5n ki\u1ec3u d\u1eef li\u1ec7u, v\u00e0 t\u1eadp h\u1ee3p c\u00e1c b\u1ea3n ghi.",
    "Tuy nhi\u00ean 1 DF c\u00f3 th\u1ec3 \u0111\u1ea1i di\u1ec7n cho l\u01b0\u1ee3ng d\u1eef li\u1ec7u l\u1edbn h\u01a1n r\u1ea5t nhi\u1ec1u so v\u1edbi c\u00e1c b\u1ea3ng trong DB, b\u1ea3n th\u00e2n m\u00ecnh \u0111\u00e3 x\u1eed l\u00fd c\u00e1c DF l\u00ean t\u1edbi h\u00e0ng ch\u1ee5c t\u1ec9 row.",
    "C\u00e1c \u0111\u1eb7c \u0111i\u1ec3m bao g\u1ed3m:",
    "immutable: t\u00ednh b\u1ea5t bi\u1ebfn, d\u1eef li\u1ec7u c\u1ee7a 1 DF sau khi t\u1ea1o ra s\u1ebd kh\u00f4ng thay \u0111\u1ed5i, n\u1ebfu mu\u1ed1n ch\u1ec9nh s\u1eeda ta c\u1ea7n t\u1ea1o ra DF m\u1edbi t\u1eeb DF ban \u0111\u1ea7u, th\u00f4ng qua DF api.",
    "rows: l\u00e0 \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\u1ea1i di\u1ec7n cho 1 b\u1ea3n ghi d\u1eef li\u1ec7u.",
    "1 DF = t\u1eadp c\u00e1c row ph\u00e2n t\u00e1n",
    "set of columns has name and an associated type: \u00dd n\u00f3i v\u1ec1 vi\u1ec7c d\u1eef li\u1ec7u c\u1ee7a DF l\u00e0 c\u00f3 c\u1ea5u tr\u00fac, g\u1ed3m t\u00ean l\u00e0 ki\u1ec3u d\u1eef li\u1ec7u.",
    "Spark SQL h\u1ed7 tr\u1ee3 r\u1ea5t nhi\u1ec1u ngu\u1ed3n d\u1eef li\u1ec7u nh\u01b0 file, DB\u2026.",
    "V\u1edbi DataFrameReader, b\u1ea1n c\u00f3 th\u1ec3 d\u1ec5 d\u00e0ng \u0111\u1ecdc d\u1eef li\u1ec7u t\u1eeb nhi\u1ec1u ngu\u1ed3n, nhi\u1ec1u \u0111\u1ecbnh d\u1ea1ng \u0111\u1ec3 r\u1ea1o ra 1 DataFrame, t\u1eeb \u0111\u00f3 c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng c\u00e1c API c\u1ee7a Spark SQL t\u01b0\u01a1ng t\u00e1c v\u1edbi ch\u00fang.",
    "DataFrame or DataSet B\u1ecf qua v\u1ec1 RDD do n\u00f3 n\u1eb1m \u1edf t\u1ea7ng Spark Core, DataFrame \u0111\u01b0\u1ee3c gi\u1edbi thi\u1ec7u v\u00e0o n\u0103m 2013 v\u00e0 DataSet l\u00e0 2015.",
    "Sau khi t\u1ed3n t\u1ea1i \u0111\u1ed9c l\u1eadp \u0111\u01b0\u1ee3c 1 n\u0103m, t\u1edbi 2016, Spark 2.0 ra m\u1eaft v\u00e0 g\u1ed9p DF v\u00e0 DS l\u1ea1i, ch\u1ec9 c\u00f2n duy nh\u1ea5t DS.",
    "C\u1ea3 DF v\u00e0 DS \u0111\u1ec1u l\u00e0 t\u1eadp d\u1eef li\u1ec7u trong Spark SQL, tuy nhi\u00ean \u0111i\u1ec3m kh\u00e1c bi\u1ec7t l\u00e0 c\u00e1c b\u1ea3n ghi trong DF \u0111\u01b0\u1ee3c fix lu\u00f4n l\u00e0 \u0111\u1ed1i t\u01b0\u1ee3ng Row, c\u00f2n DS th\u00ec c\u00f3 th\u1ec3 t\u00f9y ch\u1ec9nh \u0111\u01b0\u1ee3c ki\u1ec3u d\u1eef li\u1ec7u c\u1ee7a b\u1ea3n ghi th\u00f4ng qua \u0111\u1ecbnh ngh\u0129a case class.",
    "DataFrame = Dataset [Row] (Untyped API)",
    "DataSet = Dataset [Type]",
    "M\u1ed9t nh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a DF l\u00e0 do ki\u1ec3u d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c fix l\u00e0 row v\u00e0 truy c\u1eadp d\u1eef li\u1ec7u trong DF th\u00f4ng qua row name n\u00ean n\u1ebfu c\u00f3 sai s\u00f3t trong vi\u1ec7c truy\u1ec1n t\u00ean c\u1ed9t, tr\u00ecnh bi\u00ean d\u1ecbch s\u1ebd kh\u00f4ng th\u1ec3 ph\u00e1t hi\u1ec7n ra l\u1ed7i m\u00e0 khi th\u1ef1c thi m\u1edbi c\u00f3 exeption (Runtime exception).",
    "DS c\u00f3 th\u1ec3 kh\u1eafc ph\u1ee5c nh\u01b0\u1ee3c \u0111i\u1ec3m n\u00e0y do c\u00f3 s\u1eb5n \u0111\u1ecbnh ngh\u0129a c\u1ee7a 1 b\u1ea3n ghi, d\u1eef li\u1ec7u trong DS truy c\u1eadp th\u00f4ng qua member c\u1ee7a case class ch\u1ee9 kh\u00f4ng ph\u1ea3i truy\u1ec3n t\u00ean nh\u01b0 DF.",
    "Spark Catalyst Optimizer Nh\u01b0 c\u00e1c b\u1ea1n \u0111\u00e3 bi\u1ebft, Spark SQL c\u00f3 2 th\u00e0nh ph\u1ea7n ch\u00ednh:",
    "DataFrame API, cung c\u1ea5p c\u00e1c h\u00e0m h\u1ed7 tr\u1ee3 t\u00ednh to\u00e1n v\u00e0 x\u1eed l\u00fd d\u1eef li\u1ec7u c\u00f3 c\u1ea5u tr\u00fac ph\u00e2n t\u00e1n.",
    "Catalyst Optimizer: B\u1ed9 t\u1ed1i \u01b0u th\u1ef1c thi.",
    "Trong h\u1ea7u h\u1ebft c\u00e1c b\u00e0i test, Spark SQL lu\u00f4n c\u00f3 hi\u1ec7u n\u0103ng r\u1ea5t t\u1ed1t, m\u1ed9t trong nh\u1eefng l\u00fd do \u0111\u1ec3 c\u00f3 \u0111\u01b0\u1ee3c \u0111i\u1ec1u n\u00e0y l\u00e0 do SparkSQL c\u00f3 module t\u1ed1i \u01b0u th\u1ef1c thi Catalyst Optimizer.",
    "Catalyst Optimizer s\u1ebd nh\u01b0 m\u1ed9t b\u01b0\u1edbc b\u1ea3o \u0111\u1ea3m s\u1ebd lo\u1ea1i b\u1ecf b\u1edbt s\u1ef1 ch\u01b0a t\u1ed1i \u01b0u \u0111\u00f3 1 c\u00e1ch ch\u1ee7 \u0111\u1ed9ng, th\u00f4ng qua t\u1eadp rule-base v\u00e0 cost-base \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng s\u1eb5n.",
    "Nh\u01b0 h\u00ecnh tr\u00ean c\u00f3 th\u1ec3 th\u1ea5y, Catalyst Optimizer \u0111\u00f3ng vai tr\u00f2 trung gian, m\u1ed9t b\u00ean l\u00e0 code c\u1ee7a ng\u01b0\u1eddi d\u00f9ng, c\u00e1c t\u01b0\u01a1ng t\u00e1c \u0111\u1ecdc/ ghi/ t\u00ednh to\u00e1n v\u1edbi d\u1eef li\u1ec7u th\u00f4ng qua DataFrame API, m\u1ed9t b\u00ean l\u00e0 nh\u1eefng g\u00ec s\u1ebd \u0111\u01b0\u1ee3c th\u1ef1c thi tr\u00ean c\u00e1c executor.",
    "N\u00f3i ng\u1eafn g\u1ecdn, Catalyst Optimizer c\u00f3 th\u1ec3 s\u1ebd \u201cch\u1ec9nh s\u1eeda l\u1ea1i\u201d code c\u1ee7a b\u1ea1n tr\u01b0\u1edbc khi th\u1ef1c thi \u0111\u1ea3m b\u1ea3o vi\u1ec7c t\u00ednh to\u00e1n hi\u1ec7u qu\u1ea3 nh\u1ea5t m\u00e0 v\u1eabn \u0111\u1ea3m b\u1ea3o k\u1ebft qu\u1ea3 nh\u01b0 b\u1ea1n mong mu\u1ed1n.",
    "Try out SparkSQL and DataFrame",
    "B\u1eaft \u0111\u1ea7u nh\u00e9, trong notebook, s\u1eed d\u1ee5ng \u0111o\u1ea1n code sau \u0111\u1ec3 t\u1ea1o d\u1eef li\u1ec7u sample, \u1edf \u0111\u00e2y m\u00ecnh t\u1ea1o 1 DF c\u00f3 4 tr\u01b0\u1eddng name, gender, weight, graduation_year v\u1edbi 7 b\u1ea3n ghi:",
    "//Prepare data",
    "case class Student(name:String, gender:String, weight:Int, graduation_year:Int)",
    "val df = Seq( Student(\" John\",\" M\", 180, 2015), Student(\" Mary\",\" F\", 110, 2015), Student(\" Derek\",\" M\", 200, 2015), Student(\" Julie\",\" F\", 109, 2015), Student(\" Allison\",\" F\", 105, 2015), Student(\" kirby\",\" F\", 115, 2016), Student(\" Jeff\",\" M\", 195, 2016)). toDF",
    "How Spark run on a Cluster The Architecture of a Spark Application",
    "The Spark Driver: Trong m\u00f4 h\u00ecnh master \u2014 slave th\u00ec Spark Driver ch\u00ednh l\u00e0 Master, n\u00f3 ch\u1ee9a Spark Session, th\u1ee9 m\u00e0 gi\u00fap c\u00e1c b\u1ea1n t\u01b0\u01a1ng t\u00e1c v\u1edbi h\u00e0ng ch\u1ee5c, h\u00e0ng tr\u0103m m\u00e1y t\u00ednh kh\u00e1c \u0111\u1ec3 giao c\u00e1c c\u00f4ng vi\u1ec7c t\u00ednh to\u00e1n.",
    "Nh\u1eefng t\u00ednh to\u00e1n kh\u00f4ng th\u00f4ng qua RDD ho\u1eb7c DataFrame s\u1ebd \u0111\u01b0\u1ee3c th\u1ef1c thi tr\u00ean Driver.",
    "The Spark Executors: Trong th\u1ef1c t\u1ebf, 1 ch\u01b0\u01a1ng tr\u00ecnh spark ch\u1ec9 c\u00f3 1 Driver nh\u01b0ng l\u1ea1i c\u00f3 th\u1ec3 c\u00f3 \u0111\u1ebfn h\u00e0ng tr\u0103m, h\u00e0ng ngh\u00ecn executor, ch\u00fang l\u00e0m nhi\u1ec7m v\u1ee5 t\u00ednh to\u00e1n v\u00e0 tr\u1ea3 v\u1ec1 k\u1ebft qu\u1ea3 cho Driver.",
    "The Cluster Manager: ngh\u0129 1 ch\u00fat nh\u00e9, l\u1ea1i l\u00e0 v\u1ec1 ph\u00e2n t\u00e1n, n\u1ebfu th\u00f4ng th\u01b0\u1eddng b\u1ea1n ch\u1ea1y 1 \u1ee9ng d\u1ee5ng, h\u1ec7 \u0111i\u1ec1u h\u00e0nh s\u1ebd cung c\u1ea5p ram, cpu, data cho ti\u1ebfn tr\u00ecnh c\u1ee7a b\u1ea1n, tuy nhi\u00ean do Spark ch\u1ea1y ph\u00e2n t\u00e1n tr\u00ean nhi\u1ec1u m\u00e1y t\u00ednh n\u00ean ta c\u1ea7n 1 \u201cai \u0111\u00f3\u201d \u0111\u1ee9ng ra \u0111i\u1ec1u ph\u1ed1i t\u00e0i nguy\u00ean gi\u1eefa c\u00e1c m\u00e1y n\u00e0y, \u0111\u00f3 ch\u00ednh l\u00e0 Cluster Manager.",
    "B\u1ea3n th\u00e2n Cluster Managerc\u0169ng c\u00f3 m\u00f4 h\u00ecnh master \u2014 slave nh\u01b0 sau:",
    "Spark \u0111ang h\u1ed7 tr\u1ee3 c\u00e1c lo\u1ea1i cluster manager nh\u01b0 sau:",
    "Hadoop YARN, Apache Mesos, Kubernetes, Spark Standalone",
    "Execution Modes",
    "Spark cung c\u1ea5p s\u1eb5n cho ng\u01b0\u1eddi d\u00f9ng spark-submit",
    "D\u00f9ng \u0111\u1ec3 l\u1ef1a ch\u1ecdn c\u00e1ch m\u00e0 b\u1ea1n \u201c\u0111\u1eb7t\u201d c\u00e1c th\u00e0nh ph\u1ea7n c\u1ee7a ch\u01b0\u01a1ng tr\u00ecnh Spark nh\u01b0 Driver hay Executor \u1edf \u0111\u00e2u.",
    "C\u00f3 3 l\u1ef1a ch\u1ecdn:Cluster mode, Client mode, Local mode",
    "Cluster mode: Trong ch\u1ebf \u0111\u1ed9 n\u00e0y, to\u00e0n b\u1ed9 Spark Driver v\u00e0 Spark Executor s\u1ebd \u0111\u01b0\u1ee3c Cluster Manager qu\u1ea3n l\u00fd, n\u00f3i c\u00e1ch kh\u00e1c, Cluster Manager s\u1ebd coi Driver v\u00e0 Executor \u0111\u1ec1u trong container c\u1ee7a ch\u00fang.",
    "Client mode: \u1ede ch\u1ebf \u0111\u1ed9 n\u00e0y, Cluster Manager ch\u1ec9 qu\u1ea3n l\u00fd Executor, Driver s\u1ebd s\u1eed d\u1ee5ng t\u00e0i nguy\u00ean tr\u00ean m\u00e1y t\u00ednh m\u00e0 ng\u01b0\u1eddi d\u00f9ng submit job.",
    "Local mode: Ng\u01b0\u1ee3c l\u1ea1i vs 2 c\u00e1ch tr\u00ean, Local mode s\u1ebd ch\u1ea1y c\u1ea3 Spark Driver v\u00e0 Executor tr\u00ean m\u00e1y t\u00ednh c\u1ee7a b\u1ea1n, \u0111\u00e2y l\u00e0 c\u00e1ch \u0111\u01a1n gi\u1ea3n \u0111\u1ec3 c\u00f3 m\u00f4i tr\u01b0\u1eddng h\u1ecdc v\u00e0 th\u1ef1c h\u00e0nh Spark.",
    "The Life Cycle of a Spark Application",
    "V\u00f2ng \u0111\u1eddi 1 \u1ee9ng d\u1ee5ng Spak c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c chia l\u00e0 4 giai \u0111o\u1ea1n nh\u01b0 sau:",
    "Client Request, Launch, Execution, Completion",
    "Client request:",
    "M\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh b\u1eaft \u0111\u1ea7u khi b\u1ea1n submit, v\u00ed d\u1ee5 1 g\u00f3i jar ch\u1ee9a code c\u1ea7n th\u1ef1c thi ph\u00e2n t\u00e1n s\u1eed d\u1ee5ng th\u01b0 vi\u1ec7n Apache Spark, t\u00f9y t\u1eebng mode \u1edf tr\u00ean m\u00e0 s\u1ebd c\u00f3 1 ch\u00fat kh\u00e1c bi\u1ec7t, tuy nhi\u00ean c\u00f3 th\u1ec3 t\u1ed5ng quan nh\u01b0 h\u00ecnh sau:",
    "Spark submit s\u1ebd contact v\u1edbi master c\u1ee7a Cluster Manager, y\u00eau c\u1ea7u t\u1ea1o Spark Driver (Spark Session s\u1ebd l\u00e0 1 object trong Driver) sau \u0111\u00f3, gi\u1ea3 s\u1eed v\u1edbi mode cluster, Cluster Manager s\u1ebd t\u1ea1o c\u00e1c Container t\u01b0\u01a1ng \u1ee9ng cho Spark Driver.",
    "C\u00fa ph\u00e1p cho Spark Submit nh\u01b0 sau:",
    "./bin/spark-submit \\",
    "--class <main-class> \\",
    "--master <master-url> \\",
    "--deploy-mode cluster \\",
    "--conf <key>=<value> \\...",
    "# other options<application-jar> \\[application-arguments]",
    "Launch",
    "Khi c\u00f3 Spark Driver, \u1ee9ng d\u1ee5ng s\u1ebd t\u00f9y theo c\u1ea5u h\u00ecnh m\u00e0 y\u00eau c\u1ea7u c\u1ea5p ph\u00e1t th\u00eam t\u00e0i nguy\u00ean \u0111\u1ec3 kh\u1edfi ch\u1ea1y Spark Executor.",
    "Execution",
    "\u0110\u00e2y l\u00e0 l\u00fac th\u1ef1c thi logic tr\u00ean t\u00e0i nguy\u00ean \u0111\u00e3 xin c\u1ea5p ph\u00e1t, c\u00e1c logic map, reduce, filter s\u1ebd ch\u1ea1y \u1edf b\u01b0\u1edbc n\u00e0y.",
    "Do \u0111\u00f3ng vai tr\u00f2 \u0111i\u1ec1u ph\u1ed1i, Driver th\u01b0\u1eddng xuy\u00ean contact v\u1edbi Executor \u0111\u1ec3 giao task ho\u1eb7c c\u1eadp nh\u1eadt tr\u1ea1ng th\u00e1i, b\u1ea3n th\u00e2n c\u00e1c Executor c\u0169ng contact v\u1edbi nhau khi c\u1ea7n move d\u1eef li\u1ec7u n\u1ebfu c\u1ea7n.",
    "Completion",
    "L\u00e0 giai \u0111o\u1ea1n cu\u1ed1i c\u00f9ng, khi c\u00e1c t\u00ednh to\u00e1n \u0111\u00e3 xong ho\u1eb7c v\u00ec 1 l\u00fd do n\u00e0o \u0111\u00f3 Spark Driver down, Cluster Manager s\u1ebd th\u1ef1c hi\u1ec7n thu h\u1ed3i l\u1ea1i c\u00e1c t\u00e0i nguy\u00ean \u0111\u00e3 c\u1ea5p cho Executor.",
    "RDD: Low-level APIs",
    "Khi n\u00e0o s\u1eed d\u1ee5ng API c\u1ea5p th\u1ea5p",
    "\u2022 C\u1ea7n m\u1ed9t s\u1ed1 ch\u1ee9c n\u0103ng kh\u00f4ng c\u00f3 s\u1eb5n trong API c\u1ea5p cao h\u01a1n V\u00ed d\u1ee5: ki\u1ec3m so\u00e1t r\u1ea5t ch\u1eb7t ch\u1ebd \u0111\u1ed1i v\u1edbi v\u1ecb tr\u00ed d\u1eef li\u1ec7u v\u1eadt l\u00fd tr\u00ean to\u00e0n b\u1ed9 c\u1ee5m",
    "\u2022 Duy tr\u00ec m\u1ed9t s\u1ed1 c\u01a1 s\u1edf m\u00e3 k\u1ebf th\u1eeba \u0111\u01b0\u1ee3c vi\u1ebft b\u1eb1ng RDD",
    "\u2022 Th\u1ef1c hi\u1ec7n m\u1ed9t s\u1ed1 thao t\u00e1c t\u00f9y ch\u1ec9nh v\u1edbi bi\u1ebfn \u0111\u01b0\u1ee3c chia s\u1ebb",
    "SparkContext l\u00e0 \u0111i\u1ec3m \u0111\u1ea7u v\u00e0o cho ch\u1ee9c n\u0103ng API c\u1ea5p th\u1ea5p N\u00f3 \u0111\u01b0\u1ee3c truy c\u1eadp th\u00f4ng qua SparkSession, \u0111\u00e2y l\u00e0 c\u00f4ng c\u1ee5 \u0111\u1ec3 t\u00ednh to\u00e1n tr\u00ean m\u1ed9t c\u1ee5m Spark.",
    "RDD \u0111\u1ea1i di\u1ec7n cho m\u1ed9t t\u1eadp h\u1ee3p c\u00e1c b\u1ea3n ghi \u0111\u01b0\u1ee3c ph\u00e2n v\u00f9ng, b\u1ea5t bi\u1ebfn c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c v\u1eadn h\u00e0nh song song",
    "RDD l\u00e0 API ch\u00ednh trong s\u00ea-ri Spark 1.X \u2022 Ch\u00fang v\u1eabn c\u00f3 s\u1eb5n trong 2.X, nh\u01b0ng kh\u00f4ng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng ph\u1ed5 bi\u1ebfn",
    "M\u1ea1nh m\u1ebd nh\u01b0ng kh\u00f4ng ph\u1ea3i kh\u00f4ng c\u00f3 v\u1ea5n \u0111\u1ec1 ti\u1ec1m \u1ea9n.",
    "\u2713 C\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng n\u00e0y c\u00f3 th\u1ec3 l\u01b0u tr\u1eef b\u1ea5t k\u1ef3 th\u1ee9 g\u00ec \u1edf b\u1ea5t k\u1ef3 \u0111\u1ecbnh d\u1ea1ng n\u00e0o.",
    "\uf0fb M\u1ecdi thao t\u00e1c v\u00e0 t\u01b0\u01a1ng t\u00e1c gi\u1eefa c\u00e1c gi\u00e1 tr\u1ecb ph\u1ea3i \u0111\u01b0\u1ee3c x\u00e1c \u0111\u1ecbnh b\u1eb1ng tay.",
    "\uf0fb Vi\u1ec7c t\u1ed1i \u01b0u h\u00f3a s\u1ebd \u0111\u00f2i h\u1ecfi nhi\u1ec1u c\u00f4ng vi\u1ec7c th\u1ee7 c\u00f4ng h\u01a1n.",
    "\u2022 Spark kh\u00f4ng hi\u1ec3u c\u1ea5u tr\u00fac b\u00ean trong c\u1ee7a c\u00e1c b\u1ea3n ghi nh\u01b0 v\u1edbi c\u00e1c API c\u00f3 c\u1ea5u tr\u00fac",
    "C\u00e1c lo\u1ea1i RDD: C\u00f3 r\u1ea5t nhi\u1ec1u l\u1edbp con c\u1ee7a RDD.",
    "\u2022 H\u1ea7u h\u1ebft trong s\u1ed1 \u0111\u00f3 d\u00e0nh cho API DataFrame \u0111\u1ec3 t\u1ea1o k\u1ebf ho\u1ea1ch th\u1ef1c thi v\u1eadt l\u00fd \u0111\u01b0\u1ee3c t\u1ed1i \u01b0u h\u00f3a.",
    "\u2022 Ng\u01b0\u1eddi d\u00f9ng c\u00f3 th\u1ec3 s\u1ebd ch\u1ec9 t\u1ea1o hai lo\u1ea1i RDD: Generic RDD v\u00e0 Key-value RDD \u2022 C\u00e1c RDD kh\u00f3a-gi\u00e1 tr\u1ecb c\u00f3 c\u00e1c ho\u1ea1t \u0111\u1ed9ng \u0111\u1eb7c bi\u1ec7t c\u0169ng nh\u01b0 kh\u00e1i ni\u1ec7m v\u1ec1 ph\u00e2n v\u00f9ng t\u00f9y ch\u1ec9nh theo kh\u00f3a.",
    "Thu\u1ed9c t\u00ednh c\u1ee7a RDDs:",
    "Required: Danh s\u00e1ch c\u00e1c ph\u00e2n v\u00f9ng \u2022 M\u1ed9t ch\u1ee9c n\u0103ng \u0111\u1ec3 t\u00ednh to\u00e1n m\u1ed7i l\u1ea7n ph\u00e2n t\u00e1ch \u2022 Danh s\u00e1ch c\u00e1c ph\u1ea7n ph\u1ee5 thu\u1ed9c v\u00e0o c\u00e1c RDD kh\u00e1c",
    "Optional: Tr\u00ecnh ph\u00e2n v\u00f9ng cho RDD c\u00f3 kh\u00f3a-gi\u00e1 tr\u1ecb V\u00ed d\u1ee5: \u0111\u1ec3 n\u00f3i r\u1eb1ng RDD \u0111\u01b0\u1ee3c ph\u00e2n v\u00f9ng theo h\u00e0m b\u0103m \u2022 Danh s\u00e1ch c\u00e1c v\u1ecb tr\u00ed \u01b0u ti\u00ean \u0111\u1ec3 t\u00ednh to\u00e1n m\u1ed7i l\u1ea7n ph\u00e2n t\u00e1ch V\u00ed d\u1ee5: ch\u1eb7n c\u00e1c v\u1ecb tr\u00ed cho t\u1ec7p HDFS",
    "Kh\u1ea3 n\u0103ng l\u1eadp l\u1ecbch v\u00e0 th\u1ef1c thi ch\u01b0\u01a1ng tr\u00ecnh ng\u01b0\u1eddi d\u00f9ng c\u1ee7a Spark",
    "RDD cung c\u1ea5p c\u00e1c chuy\u1ec3n \u0111\u1ed5i v\u00e0 h\u00e0nh \u0111\u1ed9ng ho\u1ea1t \u0111\u1ed9ng gi\u1ed1ng nh\u01b0 trong DataFrames v\u00e0 DataSets.",
    "\u2022 \u0110\u00e1nh gi\u00e1: transformations lazily \u2013 actions eagerly",
    "C\u00e1c API RDD c\u00f3 s\u1eb5n b\u1eb1ng Python, Scala v\u00e0 Java \u2022 Scala v\u00e0 Java: hi\u1ec7u su\u1ea5t gi\u1ed1ng nhau cho h\u1ea7u h\u1ebft c\u00e1c ph\u1ea7n \u2022 Chi ph\u00ed l\u1edbn ph\u00e1t sinh khi thao t\u00e1c c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng th\u00f4 \u2022 Python: m\u1ed9t l\u01b0\u1ee3ng l\u1edbn hi\u1ec7u su\u1ea5t b\u1ecb m\u1ea5t \u2022 Chi ph\u00ed tu\u1ea7n t\u1ef1 h\u00f3a gi\u1eefa quy tr\u00ecnh Python v\u00e0 JVM",
    "T\u1ea1o RDD: - Create an RDD from an existing DataFrame or Dataset",
    "# in Pythonspark.range(10).rdd",
    "# in Pythonspark.range(10).toDF(\"id\").rdd.map(lambda row: row[0])",
    "\u2022 Scala and Java: from Dataset[T] to RDD[T] E.g., Dataset[Long] \u2192 RDD[Long]",
    "\u2022 Python: only from DataFrames to RDDs of type Row",
    " - Inversely, create a DataFrame or Dataset from an RDD",
    "# in Pythonspark.range(10).rdd.toDF( )",
    "T\u1ea1o RDD t\u1eeb b\u1ed9 s\u01b0u t\u1eadp c\u1ee5c b\u1ed9:",
    "- Use the parallelize method on a SparkContext to turn a single node collection into a parallel collection",
    "# in PythonmyCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \").\\words = spark.sparkContext.parallelize(myCollection, 2)",
    "- Name the RDD to show up in the Spark UI: # in Python",
    "words.setName(\"myWords\")words.name() # myWords",
    "T\u1ea1o RDD t\u1eeb text files:",
    "RDD kh\u00f4ng c\u00f3 kh\u00e1i ni\u1ec7m \u201cAPI ngu\u1ed3n d\u1eef li\u1ec7u\u201d \u2022 Ch\u00fang ch\u1ee7 y\u1ebfu x\u00e1c \u0111\u1ecbnh c\u1ea5u tr\u00fac ph\u1ee5 thu\u1ed9c v\u00e0 danh s\u00e1ch c\u00e1c ph\u00e2n v\u00f9ng.",
    "- Each record represents a line in that text file or files\\",
    "# in Python spark.sparkContext.textFile(\"/some/path/withTextFiles\")",
    "- The whole text file becomes a single record",
    "\u2022 First object: filename, second string object: content",
    "# in Pythonspark.sparkContext.wholeTextFile(\"/some/path/withTextFiles\")",
    "Transformations: Nhi\u1ec1u phi\u00ean b\u1ea3n transformation c\u1ee7a RDD c\u00f3 th\u1ec3 ho\u1ea1t \u0111\u1ed9ng tr\u00ean c\u00e1c Structured API, transformation x\u1eed l\u00fd lazily, t\u1ee9c l\u00e0 ch\u1ec9 gi\u00fap d\u1ef1ng execution plans, d\u1eef li\u1ec7u ch\u1ec9 \u0111\u01b0\u1ee3c truy xu\u1ea5t th\u1ef1c s\u1ef1 khi th\u1ef1c hi\u1ec7n action",
    "distinct: lo\u1ea1i b\u1ecf tr\u00f9ng l\u1eafp trong RDD",
    "# in Python",
    "# Spark The Definitive Guide : Big Data Processing Made Simple",
    "words.distinct( ).count( ) # 10",
    "filter: t\u01b0\u01a1ng \u0111\u01b0\u01a1ng v\u1edbi vi\u1ec7c s\u1eed d\u1ee5ng where trong SQL \u2013 t\u00ecm c\u00e1c record trong RDD xem nh\u1eefng ph\u1ea7n t\u1eed n\u00e0o th\u1ecfa \u0111i\u1ec1u ki\u1ec7n.",
    "C\u00f3 th\u1ec3 cung c\u1ea5p m\u1ed9t h\u00e0m ph\u1ee9c t\u1ea1p s\u1eed d\u1ee5ng \u0111\u1ec3 filter c\u00e1c record c\u1ea7n thi\u1ebft \u2013 Nh\u01b0 trong Python, ta c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng h\u00e0m lambda \u0111\u1ec3 truy\u1ec1n v\u00e0o filter",
    "# in Python def startsWithS(individual): return individual.startswith(\"S\")",
    "# Spark The Definitive Guide : Big Data Processing Made Simple",
    "words.filter(lambda word: startsWithS(word)).collect() # ['Spark', 'Simple']",
    "map: th\u1ef1c hi\u1ec7n m\u1ed9t c\u00f4ng vi\u1ec7c n\u00e0o \u0111\u00f3 tr\u00ean to\u00e0n b\u1ed9 RDD.",
    "Trong Python s\u1eed d\u1ee5ng lambda v\u1edbi t\u1eebng ph\u1ea7n t\u1eed \u0111\u1ec3 truy\u1ec1n v\u00e0o map",
    "words2 = words.map(lambda word: (word, word[0], word.startswith(\"S\")))",
    "words2.filter(lambda record: record[2]).take(5) # [('Spark', 'S', True), # ('Simple', 'S', True)]",
    "flatMap: cung c\u1ea5p m\u1ed9t h\u00e0m \u0111\u01a1n gi\u1ea3n h\u01a1n h\u00e0m map.",
    "Y\u00eau c\u1ea7u output c\u1ee7a map ph\u1ea3i l\u00e0 m\u1ed9t structure c\u00f3 th\u1ec3 l\u1eb7p v\u00e0 m\u1edf r\u1ed9ng \u0111\u01b0\u1ee3c.",
    "words.flatMap(lambda word: list(word)).take(5) # S, p, a, r, k",
    "sortBy: m\u00f4 t\u1ea3 m\u1ed9t h\u00e0m \u0111\u1ec3 tr\u00edch xu\u1ea5t d\u1eef li\u1ec7u t\u1eeb c\u00e1c object c\u1ee7a RDD v\u00e0 th\u1ef1c hi\u1ec7n sort \u0111\u01b0\u1ee3c t\u1eeb \u0111\u00f3.",
    "words.sortBy(lambda word: len(word) * -1).take(5) # ['Definitive', 'Processing', 'Simple', 'Spark', 'Guide']",
    "randomSplit: nh\u1eadn m\u1ed9t m\u1ea3ng tr\u1ecdng s\u1ed1 v\u00e0 t\u1ea1o m\u1ed9t random seed, t\u00e1ch c\u00e1c RDD th\u00e0nh m\u1ed9t m\u1ea3ng c\u00e1c RDD c\u00f3 s\u1ed1 l\u01b0\u1ee3ng chia theo tr\u1ecdng s\u1ed1.",
    "fiftyFiftySplit = words.randomSplit([0.5, 0.5])",
    "Action th\u1ef1c thi ngay c\u00e1c transformation \u0111\u00e3 \u0111\u01b0\u1ee3c thi\u1ebft l\u1eadp \u0111\u1ec3 thu th\u1eadp d\u1eef li\u1ec7u v\u1ec1 driver \u0111\u1ec3 x\u1eed l\u00fd ho\u1eb7c ghi d\u1eef li\u1ec7u xu\u1ed1ng c\u00e1c c\u00f4ng c\u1ee5 l\u01b0u tr\u1eef.",
    "reduce: th\u1ef1c hi\u1ec7n h\u00e0m reduce tr\u00ean RDD \u0111\u1ec3 thu v\u1ec1 1 gi\u00e1 tr\u1ecb duy nh\u1ea5t",
    "spark.sparkContext.parallelize(range(1, 21)).reduce(lambda x, y: x + y) # 210",
    "def wordLengthReducer(leftWord, rightWord):",
    "if len(leftWord) > len(rightWord):",
    "return leftWord",
    "return rightWord",
    "words.reduce(wordLengthReducer) # Undeterministic: 'Processing\u2019 or Definitive'",
    "count: \u0111\u1ebfm s\u1ed1 d\u00f2ng trong RDD words.count( )",
    "countApprox: phi\u00ean b\u1ea3n \u0111\u1ebfm x\u1ea5p x\u1ec9 c\u1ee7a count, nh\u01b0ng ph\u1ea3i cung c\u1ea5p timeout v\u00ec c\u00f3 th\u1ec3 kh\u00f4ng nh\u1eadn \u0111\u01b0\u1ee3c k\u1ebft qu\u1ea3:.",
    "confidence = 0.95",
    "timeoutMilliseconds = 400",
    "words.countApprox(timeoutMilliseconds, confidence)",
    "countByValue: \u0111\u1ebfm s\u1ed1 gi\u00e1 tr\u1ecb c\u1ee7a RDD words.countByValue( )",
    "ch\u1ec9 s\u1eed d\u1ee5ng n\u1ebfu map k\u1ebft qu\u1ea3 nh\u1ecf v\u00ec t\u1ea5t c\u1ea3 d\u1eef li\u1ec7u s\u1ebd \u0111\u01b0\u1ee3c load l\u00ean memory c\u1ee7a driver \u0111\u1ec3 t\u00ednh to\u00e1n",
    "ch\u1ec9 n\u00ean s\u1eed d\u1ee5ng trong t\u00ecnh hu\u1ed1ng s\u1ed1 d\u00f2ng nh\u1ecf v\u00e0 s\u1ed1 l\u01b0\u1ee3ng item kh\u00e1c nhau c\u0169ng nh\u1ecf.",
    "countApproxDistinct: \u0111\u1ebfm x\u1ea5p x\u1ec9 c\u00e1c gi\u00e1 tr\u1ecb kh\u00e1c nhau",
    "countByValueApprox: \u0111\u1ebfm x\u1ea5p x\u1ec9 c\u00e1c gi\u00e1 tr\u1ecb",
    "first: l\u1ea5y gi\u00e1 tr\u1ecb \u0111\u1ea7u ti\u00ean c\u1ee7a dataset",
    "max v\u00e0 min: l\u1ea7n l\u01b0\u1ee3t l\u1ea5y gi\u00e1 tr\u1ecb l\u1edbn nh\u1ea5t v\u00e0 nh\u1ecf nh\u1ea5t c\u1ee7a dataset",
    "spark.sparkContext.parallelize(1 to 20).max()",
    "take v\u00e0 c\u00e1c method t\u01b0\u01a1ng t\u1ef1: l\u1ea5y m\u1ed9t l\u01b0\u1ee3ng gi\u00e1 tr\u1ecb t\u1eeb trong RDD.",
    "take s\u1ebd tr\u01b0\u1edbc h\u1ebft scan qua m\u1ed9t partition v\u00e0 s\u1eed d\u1ee5ng k\u1ebft qu\u1ea3 \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n s\u1ed1 l\u01b0\u1ee3ng partition c\u1ea7n ph\u1ea3i l\u1ea5y th\u00eam \u0111\u1ec3 th\u1ecfa m\u00e3n s\u1ed1 l\u01b0\u1ee3ng l\u1ea5y.",
    "top v\u00e0 takeOrdered: top s\u1ebd hi\u1ec7u qu\u1ea3 h\u01a1n takeOrdered v\u00ec top l\u1ea5y c\u00e1c gi\u00e1 tr\u1ecb \u0111\u1ea7u ti\u00ean \u0111\u01b0\u1ee3c s\u1eafp x\u1ebfp ng\u1ea7m trong RDD.",
    "words.take(5) # ['Spark', 'The', 'Definitive', 'Guide', ':\u2019]",
    "words.takeOrdered(5) # [':', 'Big', 'Data', 'Definitive', 'Guide']",
    "words.top(5) # ['The', 'Spark', 'Simple', 'Processing', 'Made']",
    "takeSamples: l\u1ea5y m\u1ed9t l\u01b0\u1ee3ng gi\u00e1 tr\u1ecb ng\u1eabu nhi\u00ean trong RDD",
    "withReplacement = True",
    "numberToTake = 6",
    "randomSeed = 100",
    "words.takeSample(withReplacement, numberToTake, randomSeed)",
    "# ['Data', 'Definitive', 'Data', 'The', 'Definitive', 'Spark\u2019]",
    "# ['The', ':', 'Simple', 'Spark', 'Data', 'Big']",
    "M\u1ed9t s\u1ed1 k\u1ef9 thu\u1eadt \u0111\u1ed1i v\u1edbi RDD",
    "- L\u01b0u tr\u1eef file: words.saveAsTextFile(\"file:/tmp/bookTitle\")",
    "Th\u1ef1c hi\u1ec7n ghi v\u00e0o c\u00e1c file plain-text",
    "C\u00f3 th\u1ec3 s\u1eed d\u1ee5ng c\u00e1c codec n\u00e9n t\u1eeb th\u01b0 vi\u1ec7n c\u1ee7a Hadoop",
    "codec = \"org.apache.hadoop.io.compress.GzipCodec\"",
    "words.saveAsTextFile(\"file:/tmp/bookTitle\u201c, codec)",
    "L\u01b0u tr\u1eef v\u00e0o c\u00e1c database b\u00ean ngo\u00e0i y\u00eau c\u1ea7u ta ph\u1ea3i l\u1eb7p qua t\u1ea5t c\u1ea3 partition c\u1ee7a RDD \u2013 C\u00f4ng vi\u1ec7c \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n ng\u1ea7m trong c\u00e1c high-level API",
    "sequenceFile l\u00e0 m\u1ed9t flat file ch\u1ee9a c\u00e1c c\u1eb7p key-value, th\u01b0\u1eddng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng l\u00e0m \u0111\u1ecbnh d\u1ea1ng input/output c\u1ee7a MapReduce.",
    "Spark c\u00f3 th\u1ec3 ghi c\u00e1c sequenceFile b\u1eb1ng c\u00e1c ghi l\u1ea1i c\u00e1c c\u1eb7p key-value",
    "words.saveAsObjectFile(\"/tmp/my/sequenceFilePath\")",
    "\u0110\u1ed3ng th\u1eddi, Spark c\u0169ng h\u1ed7 tr\u1ee3 ghi nhi\u1ec1u \u0111\u1ecbnh d\u1ea1ng file kh\u00e1c nhau, cho ph\u00e9p define c\u00e1c class, \u0111\u1ecbnh d\u1ea1ng output, config v\u00e0 compression scheme c\u1ee7a Hadoop.",
    "- Caching: T\u0103ng t\u1ed1c x\u1eed l\u00fd b\u1eb1ng cache",
    "Caching v\u1edbi RDD, Dataset hay DataFrame c\u00f3 nguy\u00ean l\u00fd nh\u01b0 nhau.",
    "Ch\u00fang ta c\u00f3 th\u1ec3 l\u1ef1a ch\u1ecdn cache hay persist m\u1ed9t RDD, v\u00e0 m\u1eb7c \u0111\u1ecbnh, ch\u1ec9 x\u1eed l\u00fd d\u1eef li\u1ec7u trong b\u1ed9 nh\u1edb",
    "words.cache( ) words.persist( )",
    "words.getStorageLevel( ) # StorageLevel(False, False, False, False, 1)",
    "- Checkpointing: L\u01b0u tr\u1eef l\u1ea1i c\u00e1c b\u01b0\u1edbc x\u1eed l\u00fd \u0111\u1ec3 ph\u1ee5c h\u1ed3i",
    "Checkpointing l\u01b0u RDD v\u00e0o \u0111\u0129a c\u1ee9ng \u0111\u1ec3 c\u00e1c ti\u1ebfn tr\u00ecnh kh\u00e1c \u0111\u1ec3 th\u1ec3 s\u1eed d\u1ee5ng l\u1ea1i RDD point n\u00e0y l\u00e0m partition trung gian thay v\u00ec t\u00ednh to\u00e1n l\u1ea1i RDD t\u1eeb c\u00e1c ngu\u1ed3n d\u1eef li\u1ec7u g\u1ed1c",
    "Checkpointing c\u0169ng t\u01b0\u01a1ng t\u1ef1 nh\u01b0 cache, ch\u1ec9 kh\u00e1c nhau l\u00e0 l\u01b0u tr\u1eef v\u00e0o \u0111\u0129a c\u1ee9ng v\u00e0 kh\u00f4ng d\u00f9ng \u0111\u01b0\u1ee3c trong API c\u1ee7a DataFrame",
    "C\u1ea7n s\u1eed d\u1ee5ng nhi\u1ec1u \u0111\u1ec3 t\u1ed1i \u01b0u t\u00ednh to\u00e1n.",
    "spark.sparkContext.setCheckpointDir(\"/some/path/for/checkpointing\")",
    "words.checkpoint()",
    "WORD COUNT:",
    "text_file = sc.textFile(\u201cinput.txt\")",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\",
    ".map(lambda word: (word, 1)) \\",
    ".reduceByKey(lambda a, b: a + b)",
    "count.collect()",
    "from pyspark.sql import SQLContext",
    "sqlContext = SQLContext(sc)",
    "linesDF = sqlContext.read.text(\"input.txt\")",
    "linesDF.show(linesDF.count(),truncate=False)",
    "import pyspark.sql.functions as f",
    "wordsDF = linesDF.withColumn('word',",
    "f.explode(f.split(f.col('value'), ' ')))\\",
    ".groupBy('word')\\.count()\\.sort('count', ascending=False)\\.show()",
    "MLlib l\u00e0 th\u01b0 vi\u1ec7n m\u00e1y h\u1ecdc (ML) c\u1ee7a Spark.",
    "M\u1ee5c ti\u00eau c\u1ee7a n\u00f3 l\u00e0 l\u00e0m cho vi\u1ec7c h\u1ecdc m\u00e1y th\u1ef1c t\u1ebf c\u00f3 th\u1ec3 m\u1edf r\u1ed9ng v\u00e0 d\u1ec5 d\u00e0ng.",
    "\u1ede c\u1ea5p \u0111\u1ed9 cao, n\u00f3 cung c\u1ea5p c\u00e1c c\u00f4ng c\u1ee5 nh\u01b0:",
    "Thu\u1eadt to\u00e1n ML: c\u00e1c thu\u1eadt to\u00e1n h\u1ecdc t\u1eadp ph\u1ed5 bi\u1ebfn nh\u01b0 ph\u00e2n lo\u1ea1i, h\u1ed3i quy, ph\u00e2n c\u1ee5m v\u00e0 l\u1ecdc c\u1ed9ng t\u00e1c",
    "L\u00f4ng v\u0169: tr\u00edch xu\u1ea5t t\u00ednh n\u0103ng, bi\u1ebfn \u0111\u1ed5i, gi\u1ea3m k\u00edch th\u01b0\u1edbc v\u00e0 l\u1ef1a ch\u1ecdn",
    "\u0110\u01b0\u1eddng \u1ed1ng: c\u00f4ng c\u1ee5 \u0111\u1ec3 x\u00e2y d\u1ef1ng, \u0111\u00e1nh gi\u00e1 v\u00e0 \u0111i\u1ec1u ch\u1ec9nh \u0110\u01b0\u1eddng \u1ed1ng ML",
    "T\u00ednh b\u1ec1n b\u1ec9: l\u01b0u v\u00e0 t\u1ea3i c\u00e1c thu\u1eadt to\u00e1n, m\u00f4 h\u00ecnh v\u00e0 \u0111\u01b0\u1eddng \u1ed1ng",
    "C\u00e1c ti\u1ec7n \u00edch: \u0111\u1ea1i s\u1ed1 tuy\u1ebfn t\u00ednh, th\u1ed1ng k\u00ea, x\u1eed l\u00fd d\u1eef li\u1ec7u, v.v.",
    "Tr\u01b0\u1edbc khi ch\u00fang ta b\u1eaft \u0111\u1ea7u, vui l\u00f2ng thi\u1ebft l\u1eadp m\u00f4i tr\u01b0\u1eddng Python v\u00e0 Apache Spark tr\u00ean m\u00e1y c\u1ee7a b\u1ea1n.",
    "Truy c\u1eadp blog n\u00e0y t\u1ea1i \u0111\u00e2y \u0111\u1ec3 c\u00e0i \u0111\u1eb7t n\u1ebfu b\u1ea1n ch\u01b0a l\u00e0m nh\u01b0 v\u1eady.",
    "Ch\u00fang t\u00f4i c\u0169ng s\u1ebd s\u1eed d\u1ee5ng m\u00f4-\u0111un MLlib t\u1eeb Python trong m\u00f4i tr\u01b0\u1eddng \u1ea3o c\u1ee7a ch\u00fang t\u00f4i sau n\u00e0y \u0111\u01b0\u1ee3c t\u00edch h\u1ee3p s\u1eb5n theo m\u1eb7c \u0111\u1ecbnh v\u1edbi Spark.",
    "Apache Spark cung c\u1ea5p m\u1ed9t API H\u1ecdc m\u00e1y \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 MLlib .",
    "PySpark c\u0169ng c\u00f3 API h\u1ecdc m\u00e1y n\u00e0y b\u1eb1ng Python.",
    "N\u00f3 h\u1ed7 tr\u1ee3 c\u00e1c lo\u1ea1i thu\u1eadt to\u00e1n kh\u00e1c nhau, \u0111\u01b0\u1ee3c \u0111\u1ec1 c\u1eadp b\u00ean d\u01b0\u1edbi:",
    "mllib.classification - G\u00f3i spark.mllib h\u1ed7 tr\u1ee3 nhi\u1ec1u ph\u01b0\u01a1ng ph\u00e1p kh\u00e1c nhau \u0111\u1ec3 ph\u00e2n lo\u1ea1i nh\u1ecb ph\u00e2n, ph\u00e2n lo\u1ea1i \u0111a l\u1edbp v\u00e0 ph\u00e2n t\u00edch h\u1ed3i quy.",
    "M\u1ed9t s\u1ed1 thu\u1eadt to\u00e1n ph\u1ed5 bi\u1ebfn nh\u1ea5t trong ph\u00e2n lo\u1ea1i l\u00e0 R\u1eebng ng\u1eabu nhi\u00ean, V\u1ecbnh Naive, C\u00e2y quy\u1ebft \u0111\u1ecbnh , v.v.",
    "mllib.clustering - Clustering l\u00e0 m\u1ed9t v\u1ea5n \u0111\u1ec1 h\u1ecdc t\u1eadp kh\u00f4ng c\u00f3 gi\u00e1m s\u00e1t, theo \u0111\u00f3 b\u1ea1n nh\u1eb1m m\u1ee5c \u0111\u00edch nh\u00f3m c\u00e1c t\u1eadp con c\u1ee7a c\u00e1c th\u1ef1c th\u1ec3 v\u1edbi nhau d\u1ef1a tr\u00ean m\u1ed9t s\u1ed1 kh\u00e1i ni\u1ec7m v\u1ec1 s\u1ef1 gi\u1ed1ng nhau.",
    "mllib.fpm - \u0110\u1ed1i s\u00e1nh m\u1eabu th\u01b0\u1eddng xuy\u00ean l\u00e0 khai th\u00e1c c\u00e1c m\u1ee5c th\u01b0\u1eddng xuy\u00ean, t\u1eadp ph\u1ed5 bi\u1ebfn, chu\u1ed7i con ho\u1eb7c c\u00e1c c\u1ea5u tr\u00fac con kh\u00e1c th\u01b0\u1eddng n\u1eb1m trong s\u1ed1 c\u00e1c b\u01b0\u1edbc \u0111\u1ea7u ti\u00ean \u0111\u1ec3 ph\u00e2n t\u00edch m\u1ed9t t\u1eadp d\u1eef li\u1ec7u quy m\u00f4 l\u1edbn.",
    "\u0110\u00e2y \u0111\u00e3 l\u00e0 m\u1ed9t ch\u1ee7 \u0111\u1ec1 nghi\u00ean c\u1ee9u t\u00edch c\u1ef1c trong vi\u1ec7c khai th\u00e1c d\u1eef li\u1ec7u trong nhi\u1ec1u n\u0103m.",
    "mllib.linalg - Ti\u1ec7n \u00edch MLlib cho \u0111\u1ea1i s\u1ed1 tuy\u1ebfn t\u00ednh.",
    "mllib.recommendation - L\u1ecdc c\u1ed9ng t\u00e1c th\u01b0\u1eddng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng cho c\u00e1c h\u1ec7 th\u1ed1ng khuy\u1ebfn ngh\u1ecb.",
    "C\u00e1c k\u1ef9 thu\u1eadt n\u00e0y nh\u1eb1m m\u1ee5c \u0111\u00edch \u0111i\u1ec1n v\u00e0o c\u00e1c m\u1ee5c c\u00f2n thi\u1ebfu c\u1ee7a ma tr\u1eadn li\u00ean k\u1ebft m\u1ee5c ng\u01b0\u1eddi d\u00f9ng.",
    "spark.mllib - N\u00f3 hi\u1ec7n h\u1ed7 tr\u1ee3 l\u1ecdc c\u1ed9ng t\u00e1c d\u1ef1a tr\u00ean m\u00f4 h\u00ecnh, trong \u0111\u00f3 ng\u01b0\u1eddi d\u00f9ng v\u00e0 s\u1ea3n ph\u1ea9m \u0111\u01b0\u1ee3c m\u00f4 t\u1ea3 b\u1eb1ng m\u1ed9t t\u1eadp h\u1ee3p nh\u1ecf c\u00e1c y\u1ebfu t\u1ed1 ti\u1ec1m \u1ea9n c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n c\u00e1c m\u1ee5c nh\u1eadp b\u1ecb thi\u1ebfu.",
    "spark.mllib s\u1eed d\u1ee5ng thu\u1eadt to\u00e1n B\u00ecnh ph\u01b0\u01a1ng t\u1ed1i thi\u1ec3u xen k\u1ebd (ALS) \u0111\u1ec3 t\u00ecm hi\u1ec3u c\u00e1c y\u1ebfu t\u1ed1 ti\u1ec1m \u1ea9n n\u00e0y.",
    "mllib.regression - H\u1ed3i quy tuy\u1ebfn t\u00ednh thu\u1ed9c h\u1ecd thu\u1eadt to\u00e1n h\u1ed3i quy.",
    "M\u1ee5c ti\u00eau c\u1ee7a h\u1ed3i quy l\u00e0 t\u00ecm m\u1ed1i quan h\u1ec7 v\u00e0 s\u1ef1 ph\u1ee5 thu\u1ed9c gi\u1eefa c\u00e1c bi\u1ebfn.",
    "Giao di\u1ec7n l\u00e0m vi\u1ec7c v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy tuy\u1ebfn t\u00ednh v\u00e0 t\u00f3m t\u1eaft m\u00f4 h\u00ecnh t\u01b0\u01a1ng t\u1ef1 nh\u01b0 tr\u01b0\u1eddng h\u1ee3p h\u1ed3i quy logistic.",
    "Spark MLlib \u0111\u01b0\u1ee3c t\u00edch h\u1ee3p ch\u1eb7t ch\u1ebd tr\u00ean Spark gi\u00fap gi\u1ea3m b\u1edbt s\u1ef1 ph\u00e1t tri\u1ec3n c\u1ee7a c\u00e1c thu\u1eadt to\u00e1n h\u1ecdc m\u00e1y quy m\u00f4 l\u1edbn hi\u1ec7u qu\u1ea3 nh\u01b0 th\u01b0\u1eddng l\u00e0 l\u1eb7p \u0111i l\u1eb7p l\u1ea1i trong t\u1ef1 nhi\u00ean.",
    "C\u1ed9ng \u0111\u1ed3ng m\u00e3 ngu\u1ed3n m\u1edf c\u1ee7a Spark \u0111\u00e3 d\u1eabn \u0111\u1ebfn s\u1ef1 ph\u00e1t tri\u1ec3n nhanh ch\u00f3ng v\u00e0 vi\u1ec7c \u00e1p d\u1ee5ng Spark MLlib.",
    "C\u00f3 h\u01a1n 200 c\u00e1 nh\u00e2n t\u1eeb 75 t\u1ed5 ch\u1ee9c cung c\u1ea5p kho\u1ea3ng h\u01a1n 2000 b\u1ea3n v\u00e1 ch\u1ec9 ri\u00eang cho MLlib.",
    "MLlib d\u1ec5 tri\u1ec3n khai v\u00e0 kh\u00f4ng y\u00eau c\u1ea7u c\u00e0i \u0111\u1eb7t tr\u01b0\u1edbc, n\u1ebfu Hadoop 2 cluster \u0111\u00e3 \u0111\u01b0\u1ee3c c\u00e0i \u0111\u1eb7t v\u00e0 \u0111ang ch\u1ea1y.",
    "Kh\u1ea3 n\u0103ng m\u1edf r\u1ed9ng, t\u00ednh \u0111\u01a1n gi\u1ea3n v\u00e0 kh\u1ea3 n\u0103ng t\u01b0\u01a1ng th\u00edch ng\u00f4n ng\u1eef c\u1ee7a Spark MLlib (b\u1ea1n c\u00f3 th\u1ec3 vi\u1ebft \u1ee9ng d\u1ee5ng b\u1eb1ng Java, Scala v\u00e0 Python) gi\u00fap c\u00e1c nh\u00e0 khoa h\u1ecdc d\u1eef li\u1ec7u gi\u1ea3i quy\u1ebft c\u00e1c v\u1ea5n \u0111\u1ec1 d\u1eef li\u1ec7u l\u1eb7p l\u1ea1i nhanh h\u01a1n.",
    "C\u00e1c nh\u00e0 khoa h\u1ecdc d\u1eef li\u1ec7u c\u00f3 th\u1ec3 t\u1eadp trung v\u00e0o c\u00e1c v\u1ea5n \u0111\u1ec1 d\u1eef li\u1ec7u quan tr\u1ecdng trong khi t\u1eadn d\u1ee5ng m\u1ed9t c\u00e1ch minh b\u1ea1ch t\u1ed1c \u0111\u1ed9, s\u1ef1 d\u1ec5 d\u00e0ng v\u00e0 t\u00edch h\u1ee3p ch\u1eb7t ch\u1ebd c\u1ee7a n\u1ec1n t\u1ea3ng th\u1ed1ng nh\u1ea5t c\u1ee7a Spark.",
    "MLlib cung c\u1ea5p hi\u1ec7u su\u1ea5t cao nh\u1ea5t cho c\u00e1c nh\u00e0 khoa h\u1ecdc d\u1eef li\u1ec7u v\u00e0 nhanh h\u01a1n t\u1eeb 10 \u0111\u1ebfn 100 l\u1ea7n so v\u1edbi Hadoop v\u00e0 Apache Mahout.",
    "C\u00e1c thu\u1eadt to\u00e1n h\u1ecdc m\u00e1y Alternating Least Squares tr\u00ean Amazon \u0110\u00e1nh gi\u00e1 tr\u00ean t\u1eadp d\u1eef li\u1ec7u g\u1ed3m 660 tri\u1ec7u ng\u01b0\u1eddi d\u00f9ng, 2,4 tri\u1ec7u m\u1ee5c v\u00e0 x\u1ebfp h\u1ea1ng 3,5 B ch\u1ea1y trong 40 ph\u00fat v\u1edbi 50 n\u00fat.",
    "Vector",
    "from pyspark.ml.linalg import Vectors",
    "denseVec = Vectors.dense(1.0, 2.0, 3.0)",
    "size = 3",
    "idx = [1, 2] # locations of non-zero elements in vector",
    "values = [2.0, 3.0]",
    "sparseVec = Vectors.sparse(size, idx, values)",
    "Let\u2019s read a synthetic data set and see a sample",
    "df = spark.read.json(\"/data/simple-ml\")",
    "df.orderBy(\"value2\").show()",
    "MLlib in action: Application",
    "train, test = preparedDF.randomSplit([0.7, 0.3])",
    "from pyspark.ml.classification import LogisticRegression",
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\")",
    "print lr.explainParams()",
    "fittedLR = lr.fit(train)",
    "fittedLR.transform(train).select(\"label\", \"prediction\").show()",
    "MLlib in action: Pipelining",
    "train, test = df.randomSplit([0.7, 0.3])",
    "rForm = RFormula()",
    "lr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")",
    "from pyspark.ml import Pipeline",
    "stages = [rForm, lr]",
    "pipeline = Pipeline().setStages(stages)",
    "MLlib in action: Training",
    "from pyspark.ml.tuning import ParamGridBuilder",
    "params = ParamGridBuilder()\\",
    ".addGrid(rForm.formula, [",
    "\"lab ~ .",
    "+ color:value1\",",
    "\"lab ~ .",
    "+ color:value1 + color:value2\"])\\",
    ".addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\",
    ".addGrid(lr.regParam, [0.1, 2.0])\\",
    ".build()",
    "MLlib in action: Evaluation",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator",
    "evaluator = BinaryClassificationEvaluator()\\",
    ".setMetricName(\"areaUnderROC\")\\",
    ".setRawPredictionCol(\"prediction\")\\",
    ".setLabelCol(\"label\")",
    "from pyspark.ml.tuning import TrainValidationSplit",
    "tvs = TrainValidationSplit()\\",
    ".setTrainRatio(0.75)\\",
    ".setEstimatorParamMaps(params)\\",
    ".setEstimator(pipeline)\\",
    ".setEvaluator(evaluator)"
  ],
  "AllFileRatio": [],
  "ListAllFile": ["TailieuthiCK.docx"],
  "ListFileName": [],
  "File1Name": "TailieuthiCK.docx",
  "ListFile": []
}
